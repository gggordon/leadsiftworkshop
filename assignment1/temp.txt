brief introduct neural network david kriesel dkriesel download locat http www dkriesel scienc neural network new programm scalabl effici nn framework written java http www dkriesel tech snipe remembr dr peter kemp notari ret bonn germani small prefac origin work prepar framework seminar univers bonn germani extend present publish onlin www dkriesel first foremost provid comprehens overview subject neural network second acquir knowledg latex know mayb one day summari becom real prefac abstract work abstract yet becom prefac least littl prefac ever sinc extend text page long turn download hit ambit intent manuscript entir text written laid effect illustr illustr direct latex use xypic reflect would like see becom acquaint subject text illustr memor easi understand offer mani peopl possibl access field neural network nevertheless mathemat formal skill reader abl understand definit without read run text opposit hold reader interest subject matter everyth explain colloqui formal languag pleas let know find violat principl section text most independ document divid differ part divid chapter although chapter contain cross refer individu accesv sibl reader littl previous knowledg larger smaller chapter larger chapter provid profound insight paradigm neural network g classic neural network structur perceptron learn procedur smaller chapter give short overview explain introduct chapter addit definit explan includ excursus provid interest inform direct relat subject unfortun abl find free german sourc multi facet respect content concern paradigm neural network nevertheless written coher style aim work even could fulfil first go close gap bit bit provid easi access subject learn read code use snipe snipe well document java librari implement framework neural network speedi featur rich usabl way avail cost commerci purpos origin design high perform simul lot lot neural network even larg one train simultan recent decid give away profession refer implement cover network aspect handl within work time faster effici lot implement due origin high perform simul design goal learn use fast stabl neural network implement reason definet look snipe howev aspect cover snipe entir congruent cover manuscript kind neural network support snipe come kind neural network snipe may lot lot capabl may ever cover manuscript form practic hint anyway experi almost implement requir reader cover well snipe download page look section get start snipe find easi step step guid concern snipe document well exampl scalabl general neural inform process engin download http www dkriesel tech snipe onlin javadoc http snipe dkriesel snipe manuscript frequent incorpor snipe shade snipe paragraph like one scatter among larg part manuscript provid inform implement context snipe impli use snipe skip shade snipe paragraph snipe paragraph assum reader close look get start snipe section often class name use snipe consist differ packag omit packag name within qualifi class name sake readabl easi print manuscript text complet illustr color print monochrom color figur tabl text well chosen addit appeal design color still easi distinguish print monochrom mani tool direct integr text differ aid direct integr document make read flexibl howev anyon like prefer read word paper rather screen enjoy featur tabl content differ type chapter mark differ type chapter direct mark within tabl content chapter mark fundament definit one read almost subsequ chapter heavili depend chapter addit depend inform given preced chapter mark tabl content speak headlin throughout text short one tabl content whole manuscript pervad headlin speak headlin titl like reinforc learn central inform given associ section singl sentenc name instanc appropri headlin would reinforc learn method provid feedback network whether behav good bad howev long headlin would bloat tabl content unaccept way use short titl like first one tabl content speak one like latter throughout text margin note navig aid entir document contain margin note colloqui languag see exampl margin allow scan document quick find certain passag text includ titl new mathemat symbol mark specif margin note easi find see exampl x margin sever kind index document contain differ type index found word index open correspond page easili find search highlight text index word highlight like mathemat symbol appear sever chapter document g output neuron tri maintain consist nomenclatur regular recur element separ index mathemat symbol easili assign correspond term name person written small cap index categori person order last name term use licens begin epsilon edit text licens creativ common attribut deriv work unport licens except littl portion work licens liber licens mention main figur wikimedia common quick licens summari free redistribut document even though much better idea distribut url homepag alway contain recent version text http creativecommon org licens nd may modifi transform build upon document except person use must maintain author attribut document time may use attribut impli author endors document use lawyer bullet point summari inform conflict interpret summari actual licens actual licens alway take preced note licens extend sourc file use produc document still cite manuscript offici publish need care citat pleas find inform english german languag homepag respect subpag concern manuscript acknowledg would like express gratitud peopl contribut whatev manner success work sinc work like need mani helper first thank proofread text help reader much alphabet order wolfgang apolinarski kathrin gr paul imhoff thoma k hn christoph kunz malt lohmey joachim nock daniel plohmann daniel rosenth christian schulz tobia wilken addit thank reader dietmar berger igor buchm ller mari christ julia damaschek jochen maximilian ernestus hardi falk ann feldmeier sascha fink andrea friedmann jan gassen markus gerhard sebastian hirsch andrea hochrath nico h ft thoma ihm bori jentsch tim hussein thilo keller mario krenn mirko kunz maikel link adam maciak benjamin meier david ller andrea ller rainer penning lena reichel alexand schier matthia siegmund mathia tirtasana oliv tischler maximilian voit igor wall achim weber frank weinrei gideon maillett buij wennig philipp woock mani other feedback suggest remark http www dkriesel scienc neural network addit like thank sebastian merzbach examin work conscienti way find inconsist error particular clear lot lot languag clumsi english version especi would like thank beat kuhl translat entir text german english question made think chang phrase paragraph would particular like thank prof rolf eckmil dr nil goerk well entir divis neuroinformat depart comput scienc univers bonn made sure alway learn learn someth new neural network relat subject especi dr goerk alway will respond question abl answer write process convers prof eckmil made step back whiteboard get better overal view next global context work thank parent never get tire buy special therefor expens book alway support studi mani remark special cordial atmospher thank andrea huber tobia treutler sinc first semest rare bore would like think back school day cordial thank teacher opinion impart scientif knowledg although class particip alway wholeheart mr wilfri hartmann mr hubert peter mr frank kel furthermor would like thank whole team notari offic dr kemp dr kolb bonn alway felt good hand help keep print cost low particular christian flamm dr kemp thank go wikimedia common took imag alter suit text last least thank two peopl made outstand contribut work occupi speak place honor girlfriend verena thoma found mani mathemat logic error text discuss although lot thing christian schultz care review text spell mistak inconsist david kriesel content small prefac v biolog formal motiv philosophi histori realize neural model introduct motiv histori neural network step rule simpl applic exampl histori neural network begin golden age long silenc slow reconstruct renaiss exercis biolog neural network vertebr nervous system peripher central nervous system cerebrum cerebellum diencephalon brainstem neuron compon electrochem process neuron receptor cell various type inform process within nervous system light sens organ amount neuron live organ xiii technic neuron caricatur biolog exercis compon artifici neural network fundament concept time neural network compon neural network connect propag function network input activ threshold valu activ function common activ function output function learn strategi network topolog feedforward recurr network complet link network bias neuron repres neuron order activ synchron activ asynchron activ input output data exercis fundament learn train sampl fundament paradigm learn unsupervis learn reinforc learn supervis learn offlin onlin learn question advanc train pattern teach input use train sampl divis train set order pattern represent learn curv error measur stop learn gradient optim procedur problem gradient procedur exemplari problem boolean function pariti function spiral problem checkerboard problem ident function exemplari problem hebbian rule origin rule general form exercis ii supervis learn network paradigm perceptron backpropag variant singlelay perceptron perceptron learn algorithm converg theorem delta rule linear separ multilay perceptron backpropag error deriv boil backpropag delta rule select learn rate resili backpropag adapt weight dynam learn rate adjust rprop practic variat extens backpropag momentum term flat spot elimin second order backpropag weight decay prune optim brain damag initi configur multilay perceptron number layer number neuron select activ function initi weight encod problem relat problem exercis radial basi function compon structur inform process rbf network inform process rbf neuron analyt thought prior train train rbf network center width rbf neuron grow rbf network ad neuron limit number neuron delet neuron compar rbf network multilay perceptron exercis recurr perceptron like network depend chapter jordan network elman network train recurr network unfold time teacher forc recurr backpropag train evolut hopfield network inspir magnet structur function input output hopfield network signific weight chang state neuron generat weight matrix autoassoci tradit applic heteroassoci analog neural data storag generat heteroassoci matrix stabil heteroassoci biolog motiv heterassoci continu hopfield network exercis learn vector quantize quantize purpos lvq use codebook vector adjust codebook vector procedur learn connect neural network exercis iii unsupervis learn network paradigm self organ featur map structur function output interpret train topolog function monoton decreas learn rate neighborhood exampl topolog defect adjust resolut posit depend learn rate applic interact rbf network variat neural gas multi som multi neural gas grow neural gas exercis adapt reson theori task structur art network reson learn process pattern input top learn reson bottom learn ad output neuron extens iv excursi appendic regist excursus cluster analysi region onlin learnabl field k mean cluster k nearest neighbor nearest neighbor silhouett coeffici region onlin learnabl field structur rolf train rolf evalu rolf comparison popular cluster method initi radii learn rate multipli applic exampl exercis b excursus neural network use predict b time seri b one step ahead predict b two step ahead predict b recurs two step ahead predict b direct two step ahead predict b addit optim approach predict b chang tempor paramet b heterogen predict b remark predict share price excursus reinforc learn system structur gridworld agent environ state situat action reward return polici learn process reward strategi state valu function mont carlo method tempor differ learn action valu function q learn exampl applic td gammon car pit pole balanc reinforc learn connect neural network exercis bibliographi list figur index part biolog formal motiv philosophi histori realize neural model chapter introduct motiv histori teach comput either write fix program enabl comput learn live be programm write program develop skill execut learn without previous knowledg extern impress thus solv problem better comput today qualiti need achiev behavior devic like comput cognit adapt biolog histori develop declin resurg wide approach solv problem neural network problem categori cannot formul algorithm problem depend mani subtl factor exampl purchas price real estat brain approxim calcul without algorithm comput cannot therefor question ask learn explor problem exact learn capabl comput obvious human brain learn comput process unit memori allow comput perform complex numer calcul short time adapt compar comput brain note theoret comput power brain compris cours comparison obvious reason controversi discuss biologist comput scientist sinc respons time quantiti tell anyth qualiti perform process unit well neuron transistor cannot compar direct nevertheless comparison serv purpos indic advantag parallel mean process time brain comput process unit type process unit neuron transistor type calcul massiv parallel usual serial data storag associ address base switch time possibl switch oper actual switch oper tabl flaw comparison brain comput glanc inspir zel transistor switch time second brain contain neuron switch time second largest part brain work continu largest part comput passiv data storag thus brain parallel therefor perform close theoret maximum comput order magnitud away tabl addit comput static brain biolog neural network reorgan lifespan therefor abl learn compens error forth within text outlin use said characterist brain comput system studi artifici neural network motiv similar success work biolog system comparison overal system consist simpl numer nerv cell work massiv parallel probabl one signific aspect capabl learn need explicit program neural network instanc learn train sampl mean encourag carrot stick speak reinforc learn one result learn procedur capabl neural network general associ data success train neural network find reason solut similar problem class explicit train turn result high degre fault toler noisi input data fault toler close relat biolog neural network characterist distinct previous mention human neuron continu reorgan reorgan extern influenc neuron destroy drunken stupor type food environment influenc destroy brain cell nevertheless cognit abil signific affect thus brain toler intern error extern error often read realli dread scrawl although individu letter near imposs read modern technolog howev automat fault toler never heard someon forgot instal hard disk control comput therefor graphic card automat took task remov conductor develop communic system whole affect miss compon complet destroy disadvantag distribut fault toler storag certain fact cannot realiz first sight neural neutwork know perform fault lie usual easier perform analys convent algorithm often transfer knowledg neural network mean learn procedur caus sever error alway easi manag fault toler data hand alreadi sophist state ofth art technolog let us compar record cd scratch record audio inform spot complet lost hear pop music goe cd audio data distribut store scratch caus blurri sound vicin data stream remain larg unaffect listen notic anyth let us summar main characterist tri adapt biolog self organ learn capabl general capabl fault toler type neural network particular develop kind abil use problem class discuss cours work introductori chapter clarifi follow neural network exist differ paradigm neural network train use goal introduc paradigm supplement remark practic applic alreadi mention brain work massiv parallel contrast function comput everi compon activ time state argument massiv parallel process step rule cite step rule experi show human recogn pictur familiar object person second correspond neuron switch time second discret time step parallel process comput follow neumann architectur howev practic noth time step sequenti process assembl step cycl step look simpl applic exampl neural network simpl applic exampl let us assum small robot shown fig next page robot eight distanc sensor extract input data three sensor place front right three front left two back sensor provid real numer valu time mean alway receiv input r despit two motor need later robot simpl exampl capabl much shall drive stop might collid obstacl thus output binari h everyth okay drive h stop output call h halt signal therefor need map f r b appli input signal robot activ classic way two way realiz map one hand classic way think final result circuit small comput program realiz map easili possibl sinc exampl simpl refer technic refer sensor studi characterist curv order learn valu differ obstacl distanc emb valu aforement set rule procedur appli figur small robot eight sensor two motor arrow indic drive direct classic artifici intellig know exact rule map algorithm alway well advis follow scheme way learn hand interest success mani map problem hard comprehend straightaway way learn show differ possibl situat robot fig follow page robot shall learn cours robot life exampl robot shall simpli learn stop first treat neural network kind black box fig next page mean know structur regard behavior practic situat form simpli measur sensor valu g place robot front obstacl see illustr show robot specifi whether drive stop call train sampl thus train sampl consist exemplari input correspond desir output question transfer knowledg inform neural network figur robot posit landscap provid sensor valu differ situat add desir output valu h receiv learn sampl direct sensor orient exemplarili appli two robot figur initi regard robot control black box whose inner life unknown black box receiv eight real sensor valu map valu binari output valu sampl taught neural network use simpl learn procedur learn procedur simpl algorithm mathemat formula done everyth right chosen good sampl neural network general sampl find univers rule stop exampl option expand purpos direct control would possibl control motor robot separ sensor layout case look map f r r gradual control two motor mean sensor input thus cannot exampl stop robot let avoid obstacl difficult analyt deriv rule facto neural network would appropri goal learn sampl heart realiz principl behind ideal robot appli neural network situat abl avoid obstacl particular robot queri network continu repeat drive order contin avoid obstacl result constant cycl robot queri network consequ drive one direct chang sensor valu robot queri network chang posit sensor valu chang obvious system adapt dynam chang environ g move obstacl exampl brief histori neural network field neural network like field scienc long histori develop mani up down see soon continu style work repres histori text form compact form timelin citat bibliograph refer ad main topic discuss text citat keyword explain later mention correspond chapter histori neural network begin earli thus near simultan histori programm electron comput youth field robot call khepera less similar characterist round shape approx cm diamet two motor wheel various sensor inform recommend refer internet figur institut field neural network left right john neumann donald hebb marvin minski bernard widrow seymour papert teuvo kohonen john hopfield order appear far possibl research field comput scienc easili recogn due fact mani cite person still us begin soon warren mcculloch walter pitt introduc model neurolog network recreat threshold switch base neuron show even simpl network kind abl calcul near logic arithmet function mp furthermor first comput precursor electron brain develop among other support konrad zuse tire calcul ballist trajectori hand walter pitt warren mcculloch indic practic field applic mention work name recognit spacial pattern neural network pm donald hebb formul classic hebbian rule repres general form basi near neural learn procedur rule impli connect two neuron strengthen neuron activ time chang strength proport product two activ hebb could postul rule due absenc neurolog research abl verifi neuropsychologist karl lashley defend thesi brain inform storag realiz distribut system thesi base experi rat extent locat destroy nerv tissu influenc rat perform find way labyrinth golden age dissert marvin minski develop neurocomput snark alreadi capabl adjust weight automat never practic implement sinc capabl busili calcul nobodi realli know calcul well known scientist ambiti student dartmouth summer research project discuss put crude simul brain differ top bottom research develop earli support artifici intellig want simul capabl mean softwar support neural network want achiev system behavior imit smallest part system neuron frank rosenblatt charl wightman cowork develop first success neurocomput mark perceptron capabl recogn simpl numer mean pixel imag sensor electromechan work motor driven potentiomet potentiomet repres one variabl weight frank rosenblatt describ differ version perceptron formul verifi perceptron converg theorem describ neuron layer mimick retina threshold switch learn rule adjust connect weight bernard widrow marcian hoff introduc adalin adapt linear neuron wh fast precis adapt learn system first wide commerci use neural network could found near everi analog telephon real time adapt echo filter train mena widrow hoff rule delta rule time hoff later co founder intel corpor phd student widrow known inventor modern microprocessor one advantag delta rule origin perceptron learn algorithm adapt differ actual output correct solut larg connect weight chang larger step smaller step closer target disadvantag missappl led infinitesim small step close target follow stagnat fear scientif unpopular neural network adalin renam adapt linear element undon later learn soon weight karl steinbuch introduc technic realize associ memori seen predecessor today neural associ memori ste addit describ concept neural techniqu analyz possibl limit book learn machin nil nilsson gave overview progress work period neural network research assum basic principl self learn therefor general speak intellig system alreadi discov today assumpt seem exorbit overestim time provid high popular suffici research fund marvin minski seymour papert publish precis mathemat analysi perceptron mp show perceptron model capabl repres mani import problem keyword xor problem linear separ put overestim popular research fund implic power model would show exact problem forecast entir field would research dead result near complet declin research fund next year matter incorrect forecast today point view long silenc slow reconstruct research fund previous mention extrem short everywher research went neither confer event therefor public isol individu research provid mani independ develop neural network paradigm research discours among spite poor appreci field receiv basic theori still continu renaiss laid time teuvo kohonen introduc model linear associ model associ memori koh year model present independ neurophysiologist point view jame anderson christoph malsburg use neuron model linear biolog motiv vdm dissert harvard paul werbo develop learn procedur call backpropag error wer one decad later procedur reach today import thereaft stephen grossberg present mani paper instanc gro numer neural model analyz mathemat furthermor dedic problem keep neural network capabl learn without destroy alreadi learn associ cooper gail carpent led model adapt reson theori art teuvo kohonen describ self organ featur map koh koh known kohonen map look mechan involv self organ brain knew inform creation store genom howev enough memori structur like brain consequ brain organ creat part john hopfield invent call hopfield network hop inspir law magnet physic wide use technic applic field neural network slowli regain import fukushima miyak ito introduc neural model neocognitron could recogn handwritten charact fmi extens cognitron network alreadi develop renaiss influenc john hopfield person convinc mani research import field wide public backpropag rumelhart hinton william field neural network slowli show sign upsw john hopfield publish articl describ way find accept solut travel salesman problem use hopfield net backpropag error learn procedur general delta rule separ develop wide publish parallel distribut process group rhw linear separ problem could solv multilay perceptron marvin minski negat evalu disproven singl blow time certain kind fatigu spread field artifici intellig caus seri failur unfulfil hope time develop field research almost explos longer item result seen follow exercis exercis give one exampl follow topic book neural network neuroinformat collabor group univers work neural network softwar tool realiz neural network simul compani use neural network product servic realiz mean neural network exercis show least four applic technic neural network two field pattern recognit two field function approxim exercis briefli character four develop phase neural network give express exampl phase chapter biolog neural network biolog system solv problem system neuron work understand function differ quantiti neuron abl nervous system inform process occur short biolog overview complex simpl element neural inform process follow thought simplif order technic adapt begin describ technic side neural network would use briefli discuss biolog neural network cognit live organ reader may skip follow chapter without miss technic inform hand recommend read said excursus learn someth under neurophysiolog see small approach technic neural network caricatur natur power natur counterpart must small approach alreadi effect take brief look nervous system vertebr start rough granular proceed brain neural level read recommend book cr ksj help lot chapter vertebr nervous system entir inform process system vertebr nervous system consist central nervous system peripher nervous system first simpl subdivis realiti rigid subdivis make sens help outlin inform process bodi peripher central nervous system peripher nervous system pns compris nerv situat outsid brain spinal cord nerv form branch dens network throughout whole bodi peripher nervous system includ exampl spinal nerv pass spinal cord two within level vertebra spine suppli extrem neck trunk cranial nerv direct lead brain central nervous system cns howev main frame within vertebr place inform receiv sens organ store manag furthermor control inner process bodi last least coordin motor function organ vertebr central nervous system consist brain spinal cord fig face page howev focus brain purpos simplif divid four area fig page discuss cerebrum respons abstract think process cerebrum telencephalon one area brain chang evolut along axi run later face back head area divid two hemispher organ fold structur cerebr hemispher connect one strong nerv cord bar sever small one larg number neuron locat cerebr cortex cortex approx cm thick divid differ cortic field specif task fulfil primari cortic field respons process qualit inform manag differ percept g visual cortex respons manag vision associ cortic field howev perform abstract associ think process contain memori cerebellum control coordin motor function cerebellum locat cerebrum therefor closer spinal cord accord serv less abstract function higher prioriti larg part motor coordin perform balanc movement control error continu correct purpos cerebellum direct sensori figur illustr central nervous system spinal cord brain figur illustr brain color area brain discuss text turn abstract inform process direct reflex process darker area brain color inform muscl length well acoust visual inform furthermor receiv messag abstract motor signal come cerebrum human brain cerebellum consider smaller cerebrum rather except mani vertebr ratio less pronounc take look vertebr evolut notic cerebellum small cerebum larg least high develop structur vertebr brain two remain brain area briefli discuss diencephalon brainstem diencephalon control fundament physiolog process interbrain diencephalon includ part thalamus briefli discuss part diencephalon mediat sensori motor signal cerebrum particular thalamus decid part inform transfer cerebrum especi less import sensori percept suppress short notic avoid overload anoth part diencephalon hypothalamus control number process within bodi diencephalon heavili involv human circadian rhythm intern clock sensat pain brainstem connect brain spinal cord control reflex comparison diencephalon brainstem truncus cerebri respect phylogenet much older rough speak extend spinal cord thus connect brain spinal cord brainstem divid differ area exemplarili introduc chapter function discuss abstract function toward fundament one one import compon pon bridg kind transit station mani nerv signal brain bodi vice versa pon damag g cerebr infarct result could lockedin syndrom condit patient wall within bodi conscious awar loss cognit function cannot move communic mean sens sight hear smell tast general work perfect normal lock patient may often abl communic other blink move eye furthermor brainstem respons mani fundament reflex blink reflex cough part nervous system one thing common inform process accomplish huge accumul billion similar cell whose structur simpl communic continu larg group cell send coordin signal thus reach enorm inform process capac familiar brain leav level brain area continu cellular level bodi level neuron neuron inform process cell specifi function process within neuron give rough descript neuron function neuron noth switch inform input output switch activ enough stimuli neuron hit inform input inform output puls sent exampl neuron figur illustr biolog neuron compon discuss text compon neuron take look compon neuron fig follow way electr inform take within neuron dendrit neuron receiv inform special connect synaps synaps weight individu part inform incom signal neuron cell transfer neuron special connect synaps connect usual found dendrit neuron sometim direct soma distinguish electr chemic synaps electr synaps simpler variant electr signal receiv synaps come presynapt side direct transfer postsynapt nucleus cell thus direct strong unadjust connect signal transmitt signal receiv exampl relev shorten reaction must hard code within live organ chemic synaps distinct variant electr coupl sourc target take place coupl interrupt synapt cleft cleft electr separ presynapt side postsynapt one might think nevertheless inform flow discuss happen electr chemic process presynapt side synapt cleft electr signal convert chemic signal process induc chemic cue releas call neurotransmitt neurotransmitt cross synapt cleft transfer inform nucleus cell simpl explan later see exact work reconvert electr inform neurotransmitt degrad fast possibl releas precis inform puls spite complex function chemic synaps compar electr synaps utmost advantag one way connect chemic synaps one way connect due fact direct electr connect pre postsynapt area electr puls postsynapt area cannot flash presynapt area adjust larg number differ neurotransmitt releas various quantiti synapt cleft neurotransmitt stimul postsynapt cell nucleus other slow stimul synaps transfer strong stimul signal weak stimul one adjust vari lot one central point examin learn abil brain synaps variabl time form stronger weaker connect dendrit collect part inform dendrit branch like tree cell nucleus neuron call soma receiv electr signal mani differ sourc transfer nucleus cell amount branch dendrit call dendrit tree soma weight inform accumul cell nucleus soma receiv plenti activ stimul inhibit diminish signal synaps dendrit soma accumul signal soon accumul signal exceed certain valu call threshold valu cell nucleus neuron activ electr puls transmit neuron connect current one axon transfer outgo puls puls transfer neuron mean axon axon long slender extens soma extrem case axon stretch one meter g within spinal cord axon electr isol order achiev better conduct electr signal return point later lead dendrit transfer inform exampl neuron back begin descript neuron element axon howev transfer inform kind cell order control electrochem process neuron compon pursu path electr signal dendrit via synaps nucleus cell via axon dendrit take small step biolog toward technolog simplifi introduct electrochem inform process provid neuron maintain electr membran potenti one fundament aspect fact compar environ neuron show differ electr charg potenti membran envelop neuron charg differ charg outsid differ charg central concept import understand process within neuron differ call membran potenti membran potenti differ charg creat sever kind charg atom ion whose concentr vari within outsid neuron penetr membran insid outward find certain kind ion often less often insid descent ascent concentr call concentr gradient let us first take look membran potenti rest state neuron assum electr signal receiv outsid case membran potenti mv sinc learn potenti depend concentr gradient various ion cours central question maintain concentr gradient normal diffus predomin therefor ion eager decreas concentr gradient spread even happen membran potenti move toward mv final would membran potenti anymor thus neuron activ maintain membran potenti abl process inform work secret membran permeabl ion other maintain potenti various mechan progress time concentr gradient describ ion tri uniform distribut possibl concentr ion higher insid neuron outsid tri diffus outsid vice versa posit charg ion k potassium occur frequent within neuron less frequent outsid neuron therefor slowli diffus neuron membran anoth group negat ion collect call remain within neuron sinc membran permeabl thus insid neuron becom negat charg negat ion remain posit k ion disappear insid cell becom negat result anoth gradient electr gradient electr gradient act contrari concentr gradient intracellular charg strong therefor attract posit ion k want get back cell two gradient left alon would eventu balanc reach steadi state membran potenti mv would develop achiev rest membran potenti mv thus seem exist disturb prevent furthermor anoth import ion sodium membran permeabl howev slowli pour membran cell result sodium driven cell one hand less sodium within neuron outsid neuron hand sodium posit charg interior cell negat charg second reason sodium want get cell due low diffus sodium cell intracellular sodium concentr increas time insid cell becom less negat k pour slowli see complex mechan everyth influenc everyth sodium shift intracellular equilibrium negat less negat compar environ even two ion standstil gradient balanc could still achiev last piec puzzl get game pump rather protein atp activ transport ion direct actual take sodium activ pump cell although tri get cell along concentr gradient electr gradient potassium howev diffus strong cell activ pump back reason pump call sodium potassium pump pump maintain concentr gradient sodium well potassium sort steadi state equilibrium creat final rest potenti mv observ membran potenti maintain fact membran imperm ion ion activ pump concentr electr gradient know neuron membran potenti observ neuron receiv transmit signal neuron activ chang membran potenti learn sodium potassium diffus membran sodium slowli potassium faster move channel within membran sodium potassium channel addit perman open channel respons diffus balanc sodium potassium pump exist channel alway open respons requir sinc open channel chang concentr ion within outsid membran chang membran potenti control channel open soon accumul receiv stimulus exceed certain threshold exampl stimuli receiv neuron caus exist exampl special form neuron sensori cell light incid could stimulus incom amount light exceed threshold control channel open said threshold threshold potenti lie mv soon receiv stimuli reach valu neuron activ electr signal action potenti initi signal transmit cell connect observ neuron cell listen neuron take closer look differ stage action potenti fig next page rest state perman open sodium potassium channel permeabl membran potenti mv activ kept neuron figur initi action potenti time stimulus threshold stimulus open channel sodium intracellular charg becom posit soon membran potenti exceed threshold mv action potenti initi open mani sodium channel depolar sodium pour rememb sodium want cell lower intracellular extracellular concentr sodium addit cell domin negat environ attract posit sodium ion massiv influx sodium drastic increas membran potenti approx mv electr puls action potenti repolar sodium channel close potassium channel open posit charg ion leav posit interior cell addit intracellular concentr much higher extracellular one increas efflux ion even interior cell negat charg exterior hyperpolar sodium well potassium channel close first membran potenti slight negat rest potenti due fact potassium channel close slowli result posit charg potassium effus lower extracellular concentr refractori period ms rest state establish neuron react newli appli stimuli action potenti simpl term refractori period mandatori break neuron take order regener shorter break often neuron fire time result puls transmit axon axon puls conduct saltatori way alreadi learn axon use transmit action potenti across long distanc rememb find illustr neuron includ axon fig page axon long slender extens soma vertebr normal coat myelin sheath consist schwann cell pns oligodendrocyt cns insul axon well electr activ distanc mm gap cell schwann cell well oligodendrocyt varieti glial cell time glial cell neuron surround neuron glia glue insul provid energi etc call node ranvier said gap appear one insul cell end next one begin obvious node axon less insul may assum less insul node disadvantag axon howev node mass transfer intracellular extracellular area transfer imposs part axon situat two node internod therefor insul myelin sheath mass transfer permit generat signal similar generat action potenti within soma action potenti transfer follow continu travel along axon jump node node thus seri depolar travel along node ranvier one action potenti initi next one most even sever node activ time puls jump node node respons name puls conductor saltatori conductor obvious puls move faster jump larger axon larg internod mm achiev signal dispers approx meter second howev internod cannot grow indefinit sinc action potenti transfer would fade much reach next node node task constant amplifi signal cell receiv action potenti attach axon often connect dendrit synaps alreadi indic action potenti generat inform receiv dendrit neuron receptor cell modifi neuron action potenti generat sensori inform organ receiv environ sensori cell special receptor cell abl perceiv specif stimulus energi light temperatur sound exist certain molecul like exampl sens smell work fact sensori cell actual modifi neuron receiv electr signal via dendrit exist stimulus specif receptor cell ensur ion channel open action potenti develop process transform stimulus energi chang membran potenti call sensori transduct usual stimulus energi weak direct caus nerv signal therefor signal amplifi either transduct mean stimulus conduct apparatus result action potenti process neuron transmit thalamus alreadi learn gateway cerebr cortex therefor reject sensori impress accord current relev thus prevent abund inform manag differ receptor cell various type percept primari receptor transmit puls direct nervous system good exampl sens pain stimulus intens proport amplitud action potenti technic amplitud modul secondari receptor howev continu transmit puls puls control amount relat neurotransmitt respons transfer stimulus stimulus turn control frequenc action potenti receiv neuron process frequenc modul encod stimulus allow better perceiv increas decreas stimulus individu receptor cell cell form complex sensori organ g eye ear receiv stimuli within bodi mean interoceptor well stimuli outsid bodi mean exteroceptor outlin inform receiv environ interest look inform process inform process everi level nervous system reason believ receiv inform transmit brain process brain ensur output form motor puls thing organ actual within environ move inform process entir decentr order illustr principl take look exampl lead us abstract fundament hierarchi inform process certain inform process cerebrum develop natur inform process structur midbrain thalamus serv alreadi learn gateway cerebr cortex situat much lower hierarchi filter inform respect current relev execut midbrain import method inform process even thalamus receiv preprocess stimuli outsid let us continu lowest level sensori cell lowest level receptor cell inform receiv transfer direct process one main aspect subject prevent transmiss continu stimuli central nervous system sensori adapt due continu stimul mani receptor cell automat becom insensit stimuli thus receptor cell direct map specif stimulus energi onto action potenti depend past sensor chang sensit accord situat tast receptor respond less stimulus accord nutrit condit organ even stimulus reach receptor cell inform process alreadi execut preced signal carri apparatus exampl form amplif extern intern ear specif shape amplifi sound allow associ sensori cell sens hear sensori stimulus increas logarithm intens heard signal closer examin necessari sinc sound pressur signal ear construct vari wide exponenti rang logarithm measur advantag first overload prevent second fact intens measur intens signal less precis matter well jet fighter start next small chang nois level ignor get feel sensori organ inform process organ briefli describ usual light sens organ organ often found natur third light sens organ describ singl len eye discuss inform process eye outlin common light sens organ mani organ turn extrem use abl perceiv electromagnet radiat certain region spectrum consequ sensori organ develop detect electromagnet radiat wavelength rang radiat perceiv human eye call visibl rang simpli light differ wavelength electromagnet radiat perceiv human eye differ color visibl rang electromagnet radiat differ organ organ cannot see color wavelength rang see other even perceiv addit wavelength rang g uv rang begin human order get broader knowledg sens sight briefli look two organ sight evolutionari point view exist much longer human figur compound eye robber fli compound eye pinhol eye provid high tempor spatial resolut let us first take look call compound eye fig exampl common insect crustacean compound eye consist great number small individu eye look compound eye outsid individu eye clear visibl arrang hexagon pattern individu eye nerv fiber connect insect brain sinc individu eye distinguish obvious number pixel spatial resolut compound eye must low imag blur compound eye advantag especi fast fli insect certain compound eye process imag second human eye howev movi imag second appear fluent motion pinhol eye exampl found octopus speci work guess similar pinhol camera pinhol eye small open light entri project sharp imag onto sensori cell behind thus spatial resolut much higher compound eye due small open light entri result imag less bright singl len eye combin advantag two eye type complex light sens organ common vertebr singl lens eye result imag sharp high resolut imag environ high variabl light intens hand complex similar pinhol eye light enter open pupil project onto layer sensori cell eye retina contrast pinhol eye size pupil adapt light condit mean iri muscl expand contract pupil differ pupil dilat requir activ focus imag therefor singl len eye contain addit adjust len retina receiv inform respons inform process light signal fall eye receiv retina direct preprocess sever layer inform process cell briefli discuss differ step inform process follow way inform carri light photoreceptor receiv light signal caus action potenti differ receptor differ color compon light intens receptor real light receiv part retina sensit extent one singl photon fall retina caus action potenti sever photoreceptor transmit signal one singl bipolar cell mean inform alreadi summar final transform light signal travel sever bipolar cell ganglion cell various bipolar cell transmit inform one ganglion cell higher number photoreceptor affect ganglion cell larger field percept recept field cover ganglion less sharp imag area ganglion cell inform alreadi reduc direct retina overal imag exampl blur peripher field vision far learn inform process retina top structur take look differ kind bipolar cell well discuss would go far horizont amacrin cell cell connect front backward later allow light signal influenc later direct inform process retina much power method inform process compress blur horizont cell excit photoreceptor abl excit nearbi photoreceptor time inhibit distant bipolar cell receptor ensur clear percept outlin bright point amacrin cell intensifi certain stimuli distribut inform bipolar cell sever ganglion cell inhibit ganglion first step transmit visual inform brain show inform process first moment inform receiv hand process parallel within million inform process cell system power resist error base upon massiv divis work amount neuron live organ differ stage develop overview differ organ neural capac larg part rd neuron requir nervous system nematod worm serv popular model organ biolog nematod live soil feed bacteria neuron make ant simplifi matter neglect fact ant speci less effici nervous system due use differ attract odor ant abl engag complex social behavior form huge state million individu regard ant state individu cognit capac similar chimpanze even human neuron nervous system fli construct fli evad object real time three dimension space land upon ceil upsid consider sensori system compound eye vibrissa nerv leg much thus fli consider differenti integr calculus high dimens implement hardwar know fli easi catch cours bodili function control neuron ignor neuron enough cerebr matter creat honeybe honeybe build coloni amaz capabl field aerial reconnaiss navig neuron result mous world vertebr alreadi begin neuron suffici rat anim denounc extrem intellig often use particip varieti intellig test repres anim world rat extraordinari sens smell orient show social behavior brain frog posit within dimens frog complex build mani function swim evolv complex behavior frog continu target said fli mean eye jump threedimension space catch tongu consider probabl neuron make bat bat navig total dark room exact sever centimet use sens hear use acoust signal local self camouflag insect g moth certain wing structur reflect less sound wave echo small eat prey fli neuron requir brain companion age take look anoth popular companion neuron found cat twice much know cat eleg patient carnivor show varieti behavior way octopus posit within magnitud peopl know exampl labyrinth orient octopus vast superior rat neuron alreadi get chimpanze one anim similar human neuron make human usual human consider cognit capabl abl speak abstract rememb use tool well knowledg human develop advanc technolog manifold social structur neuron nervous system neuron human nervous system mention eleph certain whale speci state art comput abl keep aforement process power fli recent research result suggest process nervous system might vast power peopl thought long ago michaeva describ separ synaps integr inform way inform process mbw poster show right transit technic neuron neural network caricatur biolog chang biolog neural network technic one radic simplif briefli summar conclus relev technic part learn biolog neuron link weight way stimul electr transmit signal via axon axon direct transfer succeed neuron first cross synapt cleft signal chang variabl chemic process receiv neuron various input post process synapt cleft summar accumul one singl puls depend neuron stimul cumul input neuron emit puls thus output linear proport cumul input brief summari correspond exact element biolog neural network take technic approxim vectori input input technic neuron consist mani compon therefor vector natur neuron receiv puls neuron averag scalar output output neuron scalar mean neuron consist one compon sever scalar output turn form vectori input anoth neuron particular mean somewher neuron various input compon summar way one compon remain synaps chang input technic neural network input preprocess multipli number weight weight set weight repres inform storag neural network biolog origin technic adapt accumul input biolog input summar puls accord chemic chang accumul technic side often realiz weight sum get know later mean accumul continu one valu scalar instead vector linear characterist input technic neuron proport output adjust weight weight weight input variabl similar chemic process synapt cleft add great dynam network larg part knowledg neural network save weight form power chemic process synapt cleft current casual formul simpl neuron model receiv vectori input x compon xi multipli appropri weight wi accumul x wixi aforement term call weight sum nonlinear map f defin scalar output f x wixi transit specifi precis neuron model add odd end afterward take look weight adjust exercis exercis estim human brain consist approx nerv cell synaps exercis assum synaps neuron let us assum singl synaps could save bit inform veli calcul much storag capac brain note inform neuron connect neuron import chapter compon artifici neural network formal definit colloqui explan compon realiz technic adapt biolog neural network initi descript combin compon neural network chapter contain formal definit neural network compon use later text chapter abl read individu chapter work without know preced one although would use concept time neural network definit text use term time number cycl neural network respect time divid discret time step definit concept time current time present time refer next time step preced one time step refer analog follow chapter sever mathemat variabl g netj oi refer certain point time notat exampl netj oi biolog point view cours plausibl human brain neuron wait anoth one signific simplifi implement compon neural network technic neural network consist simpl process unit neuron direct weight connect neuron strength connect connect weight two neuron refer wi definit neural network neural network sort tripl v w two set v function w set neuron v set whose element call connect neuron neuron function w v r defin weight w weight connect neuron neuron shorten wi depend point view either undefin connect exist network snipe snipe instanc class neuralnetworkdescriptor creat first place descriptor object rough outlin class neural network g defin number neuron layer neural network second step descriptor object use instanti arbitrari number neuralnetwork object get start snipe program document exact two class order right thing read present layout involv descriptor depend neural network reason implement point view enabl creat maintain general paramet even larg set similar neccessarili equal network weight implement squar weight matrix w option weight vector w row number matrix indic connect begin column number matrix indic neuron target inde case numer mark exist connect matrix represent call hinton diagram neuron connect compris follow compon variabl follow path data within neuron accord fig face page top direct note cite literatur could interchang wi consist standard exist text tri use notat found frequent signific citat note cite literatur axe row could interchang publish literatur consist well propagierungsfunkt oft gewichtet summ verarbeitet eingaben netzeingab ausgabefunkt erzeugt aktivierung ausgab oft identit aktivierungsfunkt erzeugt netzeingab alter aktivierung neue aktivierung eingaben neuronen netzeingab aktivierung ausgab neuronen propag function often weight sum transform output neuron net input output function often ident function transform activ output neuron activ function transform net input sometim old activ new activ data input neuron network input activ data output neuron figur data process neuron activ function neuron impli threshold valu connect carri inform process neuron data transfer neuron via connect connect weight either excitatori inhibitori definit connect alreadi includ definit neural network snipe connect weight set use method neuralnetwork setsynaps propag function convert vector input scalar network input look neuron usual find lot neuron connect transfer output neuron propag function receiv output oi oin neuron connect transform consider connect weight wi network input netj process activ function thus network input result propag function definit propag function network input let set neuron z wiz network input call netj calcul propag function fprop follow netj fprop oi oin wi win weight sum popular multipl output neuron wi summat result netj x oi wi snipe propag function snipe implement use weight sum activ switch status neuron base model natur everi neuron certain extent time activ excit whatev call reaction neuron input valu depend activ state activ state indic extent neuron activ often short refer activ formal definit includ follow definit activ function general defin follow definit activ state activ general let neuron activ state aj short activ explicit assign indic extent neuron activ result activ function snipe possibl get set activ state neuron use method getactiv setactiv class neuralnetwork neuron get activ network input exceed treshold valu near threshold valu activ function neuron react particular sensit biolog point view threshold valu repres threshold neuron start fire threshold valu most includ definit activ function general definit follow definit threshold valu general let neuron threshold valu uniqu assign mark posit maximum gradient valu activ function activ function determin activ neuron depend network input treshold valu certain time alreadi learn activ aj neuron depend previous activ state neuron extern input definit activ function activ let neuron activ function defin aj fact netj aj transform network input netj well previous activ state aj new activ state aj threshold valu play import role alreadi mention previous activ alway relev current see exampl variant unlik variabl within neural network particular unlik one defin far activ function often defin global neuron least set neuron threshold valu differ neuron keep mind threshold valu chang exampl learn procedur particular becom necessari relat threshold valu time write instanc reason clariti omit activ function call transfer function snipe snipe activ function general neuron behavior behavior repres normal activ function even incorpor intern state dynam correspond part snipe found packag neuronbehavior contain activ function introduc next section interfac neuronbehavior allow implement custom behavior object inherit interfac pass neuralnetworkdescriptor instanc possibl defin individu behavior neuron layer common activ function simplest activ function binari threshold function fig page take two valu refer heavisid function input certain threshold function chang one valu anoth otherwis remain constant impli function differenti threshold rest deriv due fact backpropag learn exampl imposs see later popular fermi function logist function fig x map rang valu hyperbol tangent fig map function differenti fermi function expand temperatur paramet form x smaller paramet compress function x axi thus one arbitrarili approxim heavisid function incident exist activ function explicit defin depend input accord random distribut stochast activ function altern hypberbol tangent realli worth mention suggest anguita apz tire slow workstat back think make neural network propag faster quick identifi approxim function use hyperbol tangent one caus slow consequ engin approxim hyperbol tangent use two parabola piec two half line price deliv slight smaller rang valu hyperbol tangent instead depend cpu one use calcul time faster need two multipl one addit advantag mention later snipe activ function introduc implement within class fermi tangenshyperbolicus locat packag neuronbehavior fast hyperbol tangent approxim locat within class tangenshyperbolicusanguita output function may use process activ output function neuron calcul valu transfer neuron connect formal definit output function let neuron output function fout aj oj calcul output valu oj neuron activ state aj general output function defin global often function ident activ aj direct output fout aj aj oj aj unless explicit specifi differ use ident output function within text definit output function may use rang valu activ function suffici f x x heavisid function f x x fermi function temperatur paramet tanh x x hyperbol tangent figur various popular activ function top bottom heavisid binari threshold function fermi function hyperbol tangent fermi function expand temperatur paramet origin fermi function repres dark color temperatur paramet modifi fermi function order ascend steep learn strategi adjust network fit need sinc address subject later detail first get know principl neural network structur provid brief general definit definit general learn rule learn strategi algorithm use chang therebi train neural network network produc desir output given input network topolog becom acquaint composit element neural network give overview usual topolog design neural network construct network consist element everi topolog describ text illustr map hinton diagram reader immedi see characterist appli network hinton diagram dot weight repres light grey field solid one dark grey field input output arrow ad reason clariti cannot found hinton diagram order clarifi connect line neuron column neuron insert small arrow upper left cell snipe snipe design realize arbitrari network topolog respect snipe defin differ kind synaps depend sourc target kind synaps separ allow forbidden set network use setallow method neuralnetworkdescriptor instanc feedforward network consist layer connect toward follow layer feedforward text feedforward network fig follow page network first explor even use differ topolog later neuron group follow layer one input layer hidden process layer invis outsid neuron refer hidden neuron one output layer feedforward network neuron one layer direct connect neuron next layer toward output layer fig next page connect permit feedforward gfed abc gfed abc gfed abc h gfed abc h gfed abc h gfed abc gfed abc h h h h h h figur feedforward network three layer two input neuron three hidden neuron two output neuron characterist hinton diagram complet link feedforward network format block diagon network repres solid line often confront feedforward network everi neuron connect neuron next layer layer call complet link prevent name conflict output neuron often refer definit feedforward network neuron layer feedforward network fig clear separ one input layer one output layer one process layer invis outsid call hidden layer connect permit neuron follow layer gfed abc gfed abc gfed abc h gfed abc h gfed abc h gfed abc gfed abc h h h h h h figur feedforward network shortcut connect repres solid line right side feedforward block new connect ad hinton diagram shortcut connect skip layer feedforward network permit call shortcut connect fig connect skip one level connect may direct toward output layer definit feedforward network shortcut connect similar feedforward network connect may direct toward next layer toward subsequ layer recurr network influenc recurr defin process neuron influenc mean connect recurr network alway explicit defin input output neuron therefor figur omit mark concern matter number neuron direct recurr start neuron network allow neuron connect call direct recurr sometim self recurr fig face page result neuron inhibit therefor strengthen order reach activ limit definit direct recurr expand feedforward network connect neuron weight connect refer wj word diagon weight matrix w may differ indirect recurr influenc start neuron make detour connect allow toward input layer call indirect recurr neuron use indirect forward connect influenc exampl influenc neuron next layer neuron next layer influenc fig page definit indirect recurr network base feedforward network addit connect neuron preced layer allow therefor diagon w differ later recurr connect neuron within one layer connect neuron within one layer call later recurr fig page neuron often inhibit neuron layer strengthen result strongest neuron becom activ winnertak scheme v v v v v v v figur network similar feedforward network direct recurr neuron direct recurr repres solid line exact correspond diagon hinton diagram matrix x x g x g x figur network similar feedforward network indirect recurr neuron indirect recurr repres solid line see connect preced layer exist field symmetr feedforward block hinton diagram occupi k k k k figur network similar feedforward network later recurr neuron direct recurr repres solid line recurr exist within layer hinton diagram fill squar concentr around diagon height feedforward block diagon left uncov definit later recurr later recurr network permit connect within one layer complet link network allow possibl connect complet link network permit connect neuron except direct recurr furthermor connect must symmetr fig next page popular exampl self organ map introduc chapter definit complet interconnect case everi neuron alway allow connect everi neuron result everi neuron becom input neuron therefor direct recurr normal cannot appli oo oo figur complet link network symmetr connect without direct recurr hinton diagram diagon left blank clear defin layer longer exist thus matrix w may unequ everywher except along diagon bias neuron technic trick consid threshold valu connect weight know mani network paradigm neuron threshold valu indic neuron becom activ thus threshold valu activ function paramet neuron biolog point view sound plausibl complic access activ function runtim order train threshold valu threshold valu jn neuron jn realiz connect weight continu fire neuron purpos addit bias neuron whose output valu alway integr network connect neuron jn new connect get weight jn get negat threshold valu definit bias neuron neuron whose output valu alway repres gfed abc bias use repres neuron bias connect weight enabl weighttrain algorithm train bias time threshold valu neuron jn set threshold valu implement connect weight fig follow page direct train togeth connect weight consider facilit learn process word instead includ threshold valu activ function includ propag function even shorter threshold valu subtract network input part network input formal let jn neuron threshold valu jn insert bias neuron whose output valu alway generat connect said bias neuron neuron jn weight connect wbia wbia jn jn set jn receiv equival neural network whose threshold valu realiz connect weight undoubt advantag bias neuron fact much easier implement network one disadvantag represent network alreadi becom quit ugli neuron let alon great number way bias neuron often refer neuron bias neuron omit clariti follow illustr know exist threshold valu simpli treat weight snipe snipe bias neuron implement instead neuron individu bias neuron index bias neuron gfed abc gfed abc gfed abc gfed abc bias figur two equival neural network one without bias neuron left one bias neuron right neuron threshold valu found neuron connect weight connect furthermor omit weight alreadi exist connect repres dot line right side repres neuron alreadi seen either write name threshold valu neuron anoth use represent use sever time follow illustr neuron accord type data process see fig next page exampl without explan differ type neuron explain soon need take care order neuron activ calcul neural network import order individu neuron receiv process input output result distinguish two model class synchron activ neuron chang valu synchron simultan calcul network input activ output pass synchron activ wvutpqr x gau gfed abc onmlhijk wvutpqr wvutpqr tanh wvutpqr fermi onmlhijk fact gfed abc bias figur differ type neuron appear follow text correspond closest biolog counterpart implement hardwar use certain parallel comput especi feedforward network order activ generic use network arbitrari topolog definit synchron activ neuron network calcul network input time mean propag function activ mean activ function output mean output function activ cycl complet snipe implement softwar one could model general activ order everi time step calcul cach everi singl network input calcul activ exact done snipe snipe abl realiz arbitrari network topolog asynchron activ neuron chang valu simultan differ point time exist differ order introduc follow random order definit random order activ random order activ neuron random chosen neti oi updat neuron cycl fold execut step obvious neuron repeat updat one cycl other howev appar order activ alway use random permut random permut neuron chosen exact random order one cycl definit random permut initi permut neuron calcul random therefor defin order activ neuron success process order order activ well use rare first order general useless second time consum comput new permut everi cycl hopfield network chapter topolog nomin random random permut order activ note practic previous mention reason fix order activ prefer order either previous neuron activ time alreadi exist neuron activ time calcul activ taken start point topolog order definit topolog activ topolog order activ neuron updat one cycl accord fix order order defin network topolog procedur consid cyclic recurr network sinc otherwis order activ thus feedforward network procedur reason input neuron would updat first inner neuron final output neuron may save us lot time given synchron activ order feedforward network layer neuron would need full propag cycl order enabl input data influenc output network given topolog activ order need one singl propag howev everi network topolog allow find special activ order enabl save time snipe use snipe implement feedforward network may save calcul time use featur fastprop mention within document class neuralnetworkdescriptor fastprop enabl caus data propag carri slight differ way standard mode net input calcul first follow activ fastprop mode everi neuron activ calcul right net input neuron valu calcul ascend neuron index order neuron number ascend input output layer provid us perfect topolog activ order feedforward network fix order activ implement obvious fix order activ defin well therefor implement instanc feedforward network popular determin order activ accord topolog use order without verif runtim necessarili use network capabl chang topolog communic outsid world input output data neural network final let us take look fact cours mani type neural network permit input data data process produc output let us exampl regard feedforward network shown fig page two input neuron two output neuron mean two numer input x x output simplif summar input output compon input output neuron within vector x x x xn yn definit input vector network input neuron need input x x xn consid input vector x x x xn consequ input dimens refer data put neural network use compon input vector network input input neuron definit output vector network output neuron provid output ym regard output vector ym thus output dimens refer data output neural network output neuron adopt compon output vector output valu snipe order propag data neuralnetwork instanc propag method use receiv input vector array doubl return output vector way defin close examin basic compon neural network without seen network action first continu theoret explan general describ neural network could learn exercis exercis would use point view insert one bias neuron layer layer base network feedforward network discuss relat represent implement network result network chang exercis show fermi function f x well hyperbol tangent tanh x deriv express respect function two statement f x f x f x tanh x tanh x true chapter fundament learn train sampl approach thought teach machin neural network correct encourag even learn without help thought chang learn procedur chang measur error learn enough written interest characterist neural network capabl familiar problem mean train suffici train abl solv unknown problem class approach refer general introduc specif learn procedur propos basic principl learn procedur chapter differ paradigm learn learn comprehens term learn system chang order adapt g environment chang neural network could learn mani thing cours alway question implement principl neural network chang compon chang learn theoret neural network could learn develop new connect delet exist connect chang connect weight chang threshold valu neuron vari one three neuron function rememb activ function propag function output function develop new neuron delet exist neuron cours exist connect mention assum chang weight common procedur furthermor delet connect realiz addit take care connect longer train set moreov develop connect set exist connect valu connect matrix valu differ modif threshold valu refer possibl implement weight section thus perform first four learn paradigm train synapt weight chang neuron function difficult implement intuit exact biolog motiv therefor popular omit topic possibl develop delet neuron provid well adjust weight train neural network optim network topolog thus attract grow interest often realiz use evolutionari procedur sinc accept larg part learn possibl alreadi cover chang weight subject matter text howev plan extend text toward aspect train snipe method class neuralnetwork allow chang connect weight addit remov connect neuron method neuralnetworkdescriptor enabl chang neuron behavior respect activ function layer thus let neural network learn modifi connect weight accord rule formul algorithm therefor learn procedur alway algorithm easili implement mean program languag later text assum definit term desir output worth learn known defin formal train pattern train set learn sampl let train set defin follow definit train set train set name p set train pattern use train neural net introduc three essenti paradigm learn present differ regard train set unsupervis learn provid input pattern network learn aid unsupervis learn biolog plausibl method suitabl problem input pattern given network tri identifi similar pattern classifi similar categori definit unsupervis learn train set consist input pattern network tri detect similar generat pattern class refer popular exampl kohonen self organis map chapter reinforc learn method provid feedback network whether behav well bad reinforc learn network receiv logic real valu complet sequenc defin whether result right wrong intuit clear procedur effect unsupervis learn sinc network receiv specif critera problem solv definit reinforc learn train set consist input pattern complet sequenc valu return network indic whether result right wrong possibl right wrong supervis learn method provid train pattern togeth appropri desir output supervis learn train set consist input pattern well correct result form precis activ output neuron thus train set fed network output instanc direct compar correct solut network weight chang accord differ object chang weight effect network cannot associ input output pattern independ train provid plausibl result unknown similar input pattern generalis definit supervis learn train set consist input pattern correct result network receiv precis error vector return learn procedur alway biolog plausibl extrem effect therefor practic first look supervis learn procedur general text correspond follow step enter input pattern activ input neuron forward propag input network generat output compar output desir output teach input provid error vector differ vector correct network calcul base error vector correct appli offlin onlin learn must note learn offlin set train sampl present weight chang total error calcul mean error function oper simpli accumul see section onlin everi sampl present weight chang procedur advantag disadvantag discuss learn procedur section necessari offlin train procedur call batch train procedur sinc batch result correct train section whole batch train sampl includ relat chang weight valu call epoch definit offlin learn sever train pattern enter network error accumul learn pattern time definit onlin learn network learn direct error train sampl term error vector defin section mathemat formalis learn discuss question answer learn applic scheme certain requir preliminari thought question introduc check list possibl answer cours text learn input form must weight modifi allow fast reliabl learn success learn process measur object way possibl determin best learn procedur possibl predict learn procedur termin whether reach optim state finit time exampl oscil differ state learn pattern store network possibl avoid newli learn pattern destroy previous learn associ call stabil plastic dilemma see question cannot general answer discuss learn procedur network topolog individu train pattern teach input get know first learn rule need introduc teach input case supervis learn assum train set consist train pattern correspond correct output valu see output neuron train network finish train long generat wrong output output valu refer teach input neuron individu thus neuron incorrect output oj tj teach input mean correct desir output train pattern p definit train pattern train pattern input vector p compon p p pn whose desir output known enter train pattern network receiv output compar teach input desir output set train pattern call p contain finit number order pair p train pattern correspond desir output train pattern often simpli call pattern refer p literatur well text call synonym pattern train sampl etc definit teach input let output neuron teach input tj desir correct valu output input certain train pattern analog vector p teach input tn neuron combin vector alway refer specif train pattern p alreadi mention contain set p train pattern snipe class relev train data locat packag train class trainingsamplelesson allow storag train pattern teach input well simpl preprocess train data definit error vector sever output neuron differ output vector teach input train input p ep tn yn refer error vector sometim call differ vector depend whether learn offlin onlin differ vector refer specif train pattern error set train pattern normal certain way briefli summar vector yet defin input vector x enter neural network depend type network use neural network output output vector basic train sampl p noth input vector use train purpos know correspond teach input noth desir output vector train sampl error vector ep differ teach input actur output x general network oper p network train train tri bring close possibl one advic concern notat refer output valu neuron oi thus output output neuron call output valu network refer certain network output neuron output output output neuron respect true use train sampl seen learn principl step requir take look select train data learn curv success learn particular interest whether network memor whether use train sampl quit exact produc right output provid wrong answer problem class suppos network train map r b therefor use train sampl fig next page could chanc final network exact mark color area around train sampl output fig top otherwis output thus suffici storag capac concentr six train sampl output impli overs network much free storag capac hand network could insuffici capac fig bottom rough present input data correspond good general perform desir thus find balanc fig middl use divid set train sampl often propos solut problem divid train set one train set realli use train one verif set test progress figur visual train result train set network capac high top correct middl low bottom provid enough train sampl usual divis relat instanc train data verif data random chosen finish train network provid good result train data well verif data snipe method splitlesson within class trainingsamplelesson allow split trainingsamplelesson respect given ratio note verif data provid poor result modifi network structur data provid good result otherwis run risk tailor network verif data mean data includ train even use explicit train solut third set valid data use valid suppos success train train less pattern obvious withhold inform network risk worsen learn perform text exact reproduct given sampl success general approxim whole function definit use train less inform network order pattern represent find differ strategi choos order pattern present pattern present random sequenc guarante pattern learn equal well howev standard method alway sequenc pattern hand provok pattern memor use recurr network later learn type network random permut would solv problem alreadi mention timeconsum calcul permut snipe method shufflesampl locat class trainingsamplelesson permut lesson learn curv error measur learn curv indic progress error determin various way motiv creat learn curv curv indic whether network progress error normal repres distanc measur correct current output network exampl take pattern specif squar error prefactor go use deriv backpropag error let output neuron set output neuron errp x definit specif error specif error errp base singl train sampl mean generat onlin addit root mean squar abbrevi rms euclidean distanc often use euclidean distanc general theorem pythagora use lower dimens still visual use definit euclidean distanc euclidean distanc two vector defin errp sx general root mean squar common use sinc consid extrem outlier greater extent definit root mean squar root mean squar two vector defin errp sp offlin learn total error cours one train epoch interest use err x p p errp definit total error total error err base train sampl mean generat offlin analog generat total rms total euclidean distanc cours whole epoch cours possibl use type error measur get use error measur method suggest look technic report prechelt pre report error measur method sampl problem discuss simmilar suggest discuss exemplari problem snipe sever static method repres differ method error measur implement class errormeasur depend method error measur learn curv certain chang perfect learn curv look like negat exponenti function mean proport fig follow page thus represent learn curv illustr mean logarithm scale fig second diagram bottom said scale combin descend line impli exponenti descent error network good job problem difficult logarithm represent err see metaphor speak descend line often form spike bottom reach limit bit resolut comput network actual learn optimum capabl learn typic learn curv show flat area well show step sign malfunct learn process see fig well suit represent make slight decreas learn curv look good cautious read literatur stop learn big question stop learn general train stop user front learn comput think error small enough inde easi answer thus give someth think howev depend object view comparison sever learn curv confid result exampl boost network alway reach near final error rate differ random initi repeat initi train provid object result fehler epoch fehler epoch fehler epoch fehler epoch figur four illustr show ideal smooth learn curv note altern logarithm linear scale note small inaccur spike visibl sharp bend curv first second diagram bottom hand possibl curv descend fast begin longer time learn overtaken anoth curv indic either learn rate wors curv high wors curv simpli got stuck local minimum first find rememb larger error valu wors small one case note mani peopl generat learn curv respect train data surpris thing work reason object clariti forgotten plot verif data second learn curv general provid valu slight wors stronger oscil good general curv decreas network eventu begin memor sampl shape learn curv provid indic learn curv verif sampl sudden rapid rise learn curv verif data continu fall could indic memor general get poorer poorer point could decid whether network alreadi learn well enough next point two curv mayb final point learn appli procedur call earli stop remind act indic draw conclus gradient optim procedur order establish mathemat basi follow learn procedur explain briefli meant gradient descent backpropag error learn procedur exampl involv mathemat basi thus inherit advantag disadvantag gradient descent gradient descent procedur general use maxim minim dimension function due clariti illustr fig next page show two dimens princip limit number dimens gradient vector g defin differenti point function point point exact toward steepest ascent indic gradient direct mean norm g thus gradient general deriv multi dimension function accord negat gradient g exact point toward steepest descent gradient oper refer figur visual gradient descent two dimension error function move forward opposit direct g steepest descent toward lowest point step width proport g steeper descent faster step left area shown right step contour line shown obvious movement made opposit direct g toward minimum function continu slow proport g sourc http webster fhs hagenberg ac staff sdreisei teach ws patternclassif graddesc pdf nabla oper overal notat gradient g point x two dimension function f g x f x definit gradient let g gradient g vector compon defin point differenti dimension function f x x xn gradient oper notat defin g x x xn f x x xn g direct point f toward steepest ascent point g correspond degre ascent gradient descent mean go downhil small step start point function toward gradient g mean vivid speak direct ball would roll start point size step proport g steeper descent longer step therefor move slowli flat plateau steep ascent run downhil rapid came valley would depend size step jump would return figur possibl error gradient descent detect bad minima b quasi standstil small gradient oscil canyon leav good minima valley across opposit hillsid order closer closer deepest point valley walk back forth similar ball move within round bowl definit gradient descent let f dimension function sn given start point gradient descent mean go f direct g toward g step size g toward smaller smaller valu f gradient descent procedur errorless optim procedur see follow section howev work still well mani problem make optim paradigm frequent use anyway let us look potenti disadvantag keep mind bit gradient procedur incorpor sever problem alreadi impli section gradient descent therefor backpropag promis foolproof one problem result alway reveal error occur often gradient descent converg suboptim minima everi gradient descent procedur exampl get stuck within local minimum part fig preced page problem increas proport size error surfac univers solut realiti one cannot know optim minimum reach consid train success accept minimum found flat plataeus error surfac may caus train slow pass flat plateau instanc gradient becom neglig small hard descent part b fig requir mani step hypothet possibl gradient would complet stop descent even good minima reach may left afterward hand gradient larg steep slope larg step made good minimum possibl miss part fig steep canyon error surfac may caus oscil sudden altern one strong negat gradient strong posit one even result oscil part fig natur error occur often think possibl b exemplari problem allow test self code learn strategi look learn formal point view much yet littl time look exemplari problem later use test implement network learn rule tabl illustr pariti function three input boolean function popular exampl one work nineteen sixti xor function b b need hidden neuron layer discuss detail thus need least two neuron inner layer let activ function layer except input layer cours hyperbol tangent trivial expect output depend whether function xor output exact first beginn mistak occur output close close limit hyperbol tangent case fermi function need larg network input chanc reach network input larg weight learn learn process larg extend therefor wiser enter teach input desir output satisfi network output valu instead anoth favourit exampl singlelay perceptron boolean function pariti function pariti function map set bit depend whether even number input bit set basic function b b character easi learnabl approx shown tabl learn effort rapid increas reader may creat score tabl bit pariti function conspicu figur illustr train sampl spiral problem spiral problem train sampl function let us take two spiral coil fig function certain repres map r b one spiral assign output valu spiral memor help network understand map exampl solv mean mlp checkerboard problem creat two dimension function form r b specifi checker train sampl fig next page one color field repres rest repres difficulti increas proport size function field easi learn larger field difficult eventu use method suitabl kind problem mlp spiral problem similar checkerboard problem mathemat speak first problem use polar coordin instead cartesian coordin introduc exampl one last trivial case ident figur illustr train sampl checkerboard problem ident function use linear activ function ident map r r cours within paramet use activ function problem network put obstacl way use sigmoid function would difficult network learn ident tri fun time hava look first mathemat learn rule lot exemplari problem lot lot exemplari problem recommend technic report written prechelt pre name section error measur procedur hebbian learn rule basi learn rule donald hebb formul hebbian rule basi complic learn rule discuss text distinguish origin form general form kind principl learn rule origin rule definit hebbian rule neuron receiv input neuron neuron strong activ time increas weight wi strength connect mathemat speak rule wi oiaj wi chang weight proport follow factor output oi predecessor neuron well activ aj successor neuron constant learn rate discuss section chang weight wi simpli ad weight wi speak twice activ formula use oi aj output neuron neuron activ neuron rememb ident often use output function therefor oi neuron often besid hebb postul rule long specif technic neuron consid learn rule prefer binari activ clear possibl activ weight either increas remain constant sooner later would go infinitum sinc correct upward error occur compens use activ thus weight decreas activ predecessor neuron dissent one successor neuron otherwis increas longer origin version hebbian rule general form learn rule discuss special mathemat general form mr hebbian rule definit hebbian rule general general form hebbian rule specifi proport chang weight product two undefin function defin input valu wi h oi wi g aj tj thus product function g aj tj h oi wi well constant learn rate result chang weight see h receiv output predecessor cell oi well weight predecessor successor wi g expect actual desir activ successor aj tj stand aforement teach input alreadi mention g h specifi general definit therefor return path special discuss equat short pictur learn rule could look like thought learn introduc first network paradigm includ learn procedur exercis exercis calcul averag valu standard deviat follow data point p p p p p p part ii supervis learn network paradigm chapter perceptron backpropag variant classic among neural network talk neural network major case speak percepton variat perceptron multilay network without recurr fix input output layer descript perceptron limit extens avoid limit deriv learn procedur discuss problem alreadi mention histori neural network perceptron describ frank rosenblatt ros initi rosenblatt defin alreadi discuss weight sum linear activ function compon perceptron establish definit perceptron time term use describ feedforward network shortcut connect network layer scanner neuron retina static weight connect follow layer call input layer fig next page weight layer allow chang neuron subordin retina pattern detector initi use binari perceptron everi output neuron exact two possibl output valu g thus binari threshold function use activ function depend threshold valu output neuron way binari activ function repres queri negat mean negat weight perceptron thus use accomplish true logic inform process kapitel perceptron dkriesel gfed abcr oo oo oo oo oo oo oo oo gfed abc gfed abc gfed abc gfed abc wooooooooooooooooo wvutpqr gfed abc p pp pp pp pp pp pp pp pp p gfed abc gfed abc gfed abc gfed abc vnnnnnnnnnnnnnnnnnn abbildung aufbau perceptron schicht variabl verbindungen verschiedenen ansichten durchgezogen gewichtsschicht unteren beiden abbildungen trainierbar oben beispiel informationsabtastung aug mitt skizz eingezeichnet fester gewichtsschicht verwendung definierten funktionsbeschreibenden design fur neuron unten eingezeichnet fest gewichtsschicht benennung einzelnen neuronen unser konvent fest gewichtschicht weiteren verlauf arbeit mehr betrachten kriesel kleiner uberblick uber neuronal netz epsilon figur architectur perceptron one layer variabl connect differ view solid drawn weight layer two illustr bottom train left side exampl scan inform eye right side upper part draw exampl indic fix weight layer use defin design function descript neuron right side lower part without indic fix weight layer name neuron correspond convent fix weight layer longer taken account cours work whether method reason anoth matter cours easiest way achiev boolean logic illustr perceptron use simpl logic compon theoret speak boolean function realiz mean perceptron connect seri interconnect sophist way see possibl without connect serial provid definit perceptron defin type neuron use chapter definit input neuron input neuron ident neuron exact forward inform receiv thus repres ident function indic symbol therefor input neuron repres symbol gfed abc definit inform process neuron inform process neuron somehow process input inform repres ident function binari neuron sum input use weight sum propag function illustr sign activ function neuron binari threshold function illustr lead us complet depict inform process neuron name wvutpqr neuron use weight sum propag function activ function hyperbol tangent fermi function separ defin activ function fact similar repres wvutpqr tanh wvutpqr fermi onmlhijk fact neuron refer fermi neuron tanh neuron know compon perceptron abl defin definit perceptron perceptron fig face page feedforward network contain retina use data acquisit fix weight connect first neuron layer input layer fix weight layer follow least one trainabl weight layer one neuron layer complet link follow layer first layer perceptron consist input neuron defin feedforward network often contain shortcut exact correspond origin descript therefor includ definit see retina includ lower part fig matter fact first neuron layer often understood simplifi suffici method input layer layer forward input valu retina static weight behind longer mention display sinc process inform case depict perceptron start input neuron may confus reader claim definit perceptron defin perceptron follow section therefor suggest keep definit back mind take grant cours work snipe method setsettingstopologyfeedforward variat withshortcut neuralnetworkdescriptor instanc appli set descriptor appropri feedforward network feedforward network shortcut respect kind connect allow other fastprop activ singlelay perceptron provid one trainabl weight layer connect trainabl weight go input layer output neuron return inform whether pattern enter input neuron recogn thus singlelay percept abbrevi slp one level trainabl weight fig page definit singlelay perceptron singlelay perceptron slp perceptron one layer variabl weight one layer output neuron technic view slp shown fig face page certain exist sever output neuron consider chang concept perceptron fig next page perceptron sever output neuron regard sever differ perceptron input boolean function shown fig page trivial exampl easili compos know train singlelay perceptron therefor first take look perceptron learn algorithm look delta rule perceptron learn algorithm converg theorem origin perceptron learn algorithm binari neuron activ function describ alg proven algorithm converg finit time finit time perceptron learn anyth repres perceptron converg theorem ros pleas get hope soon perceptron capabl repres explor later explor linear separ problem cover fact least singlelay perceptron unfortun cannot repres lot problem gfed abc bias wbia gfed abc wi gfed abc wi figur singlelay perceptron two input neuron one output neuron network return output mean arrow leav network trainabl layer weight situat center label remind bias neuron includ although weight wbia normal weight treat like repres dot line signific increas clariti larger network futur bias neuron longer includ gfed abc p pp pp pp pp pp pp pp pp gfed abc p pp pp pp pp pp pp pp pp p gfed abc gfed abc v gfed abc w gfed abc gfed abc gfed abc figur singlelay perceptron sever output neuron gfed abc gfed abc gfed abc gfed abc gfed abc gfed abc figur two singlelay perceptron boolean function upper singlelay perceptron realiz lower one realiz activ function inform process neuron binari threshold function avail threshold valu written neuron p p error larg input p network calcul output p set train pattern output neuron output okay correct weight els input neuron wi wi oi increas weight toward oi input neuron wi wi oi decreas weight toward oi algorithm perceptron learn algorithm perceptron learn algorithm reduc weight output neuron return instead invers case increas weight delta rule gradient base learn strategi slps follow deviat binari threshold valu activ function least backpropag error need see differenti even semi linear activ function follow delta rule like backpropag deriv mr alway necessari use fact howev point appropri part work compar aforement perceptron learn algorithm delta rule advantag suitabl binari activ function far away learn target automat learn faster suppos singlelay perceptron random set weight teach function mean train sampl set train sampl call p contain alreadi defin pair p train sampl p associ teach input remind x input vector output vector neural network output neuron refer input output neuron addit defin error vector ep repres differ certain train sampl p furthermor let set output neuron set input neuron anoth name convent shall exampl output teach input addit index p may set order indic valu pattern specif sometim consider enhanc clariti learn target certain train sampl output network approxim desir output formal true p p ep mean first understand total error err function weight total error increas decreas depend chang weight w w figur exemplari error surfac neural network two trainabl connect w w general neural network two connect would made illustr complex time error surfac craggi complic search minimum definit error function error function err w r regard set weight w vector map valu onto normal output error normal otherwis error map onto one singl r perform gradient descent obvious specif error function analog generat singl pattern p alreadi shown section gradient descent procedur calcul gradient arbitrari finit dimension function error function err w move direct gradient minimum reach err w defin set weight regard vector w tri decreas minim error simpli tweak weight thus one receiv inform chang weight chang weight refer w calcul gradient err w error function err w w err w due relat proport constant equal hold soon get anoth mean real practic use beyond mere mean proport constant ask reader patient w err w follow tradit literatur previous defin w weight matrix awar conflict bother us simplifi analysi rewrit gradient error function accord weight usual partial deriv accord singl weight wi variabl weight exist hidden output layer thus tweak everi singl weight observ error function chang deriv error function accord weight wi obtain valu wi chang weight wi err w wi follow question aris error function defin exact good mani result far away desir one error function provid larg valu hand similar bad mani result close desir one exist extrem far out result squar distanc output vector teach input appear adequ need provid error errp specif train sampl p output output neuron errp w x tp yp thus calcul squar differ compon vector given pattern p sum squar summat specif error errp w pattern p yield definit error err therefor definit error function err w err w x p p errp w sum p z x p p x tp yp z sum observ reader certain wonder factor equat sudden came root equat formula look similar euclidean distanc fact result simpl pragmat intent minim error root function decreas argument simpli omit reason calcul implement effort sinc need minim similar matter term minim divid therefor allow multipli done cancel cours calcul continu deriv delta rule linear activ function alreadi discuss tweak individu weight wi bit see error err w chang correspond deriv error function err w accord weight wi deriv correspond sum deriv specif error errp accord weight sinc total error err w result sum specif error wi err w wi x p p errp w wi think question neural network process data basic data transfer function result function sent anoth one ignor output function path neuron output oi oi neuron enter neuron initi propag function weight sum network input go receiv sent activ function neuron receiv output neuron time compon output vector net fact fact net see output result mani nest function fact net fact oi wi oi wi clear could break output singl input neuron unnecessari sinc process inform slp thus calcul deriv equat due nest function appli chain rule factor deriv errp w wi equat errp w wi errp w wi let us take look first multipl factor equat preced page repres deriv specif error errp w accord output chang error errp output examin errp equat page clear show chang exact differ teach input output tp rememb sinc output neuron yp closer output teach input smaller specif error thus replac one differ call p reason name delta rule errp w wi tp wi p wi second multipl factor equat preced page follow one deriv output specif pattern p neuron accord weight wi chang weight chang due requir begin deriv linear activ function fact therefor well look chang network input wi chang errp w wi p p iwi wi result deriv p iwi wi simplifi function p iwi deriv consist mani summand summand iwi contain variabl wi accord deriv thus p iwi wi therefor errp w wi p p insert equat previous page result modif rule weight wi wi x p p p howev begin deriv intend offlin rule mean question add error pattern learn pattern repres although approach mathemat correct implement far time consum see later chapter partial need lot compuat effort train onlin learn version delta rule simpli omit summat learn realiz immedi present pattern simplifi notat longer necessarili relat pattern p wi oi version delta rule shall use follow definit definit delta rule determin analog aforement deriv function h hebbian theori equat page provid output oi predecessor neuron function g differ desir activ actual activ receiv delta rule known widrow hoff rule wi oi oi use desir output instead activ teach input therefor output function output neuron repres ident obtain wi oi oi correspond differ case delta rule chang weight output neuron proport differ current activ output correspond teach input refer factor refer delta appar delta rule appli slps sinc formula alway relat teach input teach input inner process layer neuron output tabl definit logic xor input valu shown left output valu right gfed abc wi gfed abc wi xor figur sketch singlelay perceptron shall repres xor function imposs slp capabl repres linear separ data let f xor function expect two binari input generat binari output precis definit see tabl let us tri repres xor function mean slp two input neuron one output neuron fig use weight sum propag function binari activ function threshold valu ident output function depend output valu follow hold net oi wi oi wi figur linear separ input input neuron dimension straight line b show corner belong set xor function separ assum posit weight wi inequ preced page equival oi wi oi wi constant threshold valu right part inequ straight line coordin system defin possibl output oi oi input neuron fig requir inequ posit wi output neuron fire input combin lie generat straight line negat wi would fire input combin lie straight line note four corner unit squar possibl input xor function know binari input order solv xor problem turn move straight line input set separ input set b obvious imposs figur linear separ input input neuron dimension plane general input paramet mani input neuron repres dimension cube separ slp dimension hyperplan fig set separ hyperplan linear separ classifi slp unfortun seem percentag linear separ problem rapid decreas increas see tabl face page limit function slp addit test linear separ difficult thus difficult task input need someth power slp xor problem one task sinc perceptron suppos repres xor function alreadi need hidden layer fig next page multilay perceptron contain trainabl weight layer perceptron two trainabl weight layer call multilay perceptron mlp power slp know singlelay perceptron divid number binari function lin separ one share tabl number function concern binari input number proport function thereof linear separ accord zel wid gfed abc gfed abc gfed abc gfed abc xor figur neural network realiz xor function threshold valu far exist locat within neuron input space mean hyperplan two dimension input space mean straight line two stage perceptron two trainabl weight layer three neuron layer classifi convex polygon process straight line g form recogn pattern lie straight line straight line straight line thus metaphor speak took slp sever output neuron attach anoth slp upper part fig face page multilay perceptron repres univers function approxim proven theorem cybenko cyb anoth trainabl weight layer proceed analog convex polygon ad subtract somehow process oper lower part fig next page general mathemat proven even multilay perceptron one layer hidden neuron arbitrarili precis approxim function finit mani discontinu well first deriv unfortun proof construct therefor left us find correct number neuron weight follow use widespread abbrevi form differ multilay perceptron denot two stage perceptron neuron input layer neuron hidden layer neuron output layer mlp definit multilay perceptron perceptron one layer variabl weight connect refer multilay perceptron mlp layer stage perceptron therebi exact variabl weight layer neuron layer retina disregard neuron layer input layer sinc three stage perceptron classifi set form combin separ arbitrarili mani convex polygon anoth step advantag respect function represent cautious read literatur mani differ definit count layer sourc count neuron layer count weight layer sourc includ retina trainabl weight layer exclud reason output neuron layer work chose definit provid opinion inform learn capabl use cosist rememb stage perceptron exact trainabl weight layer find summari perceptron classifi type set tabl page face challeng train perceptron one weight layer gfed abc gfed abc gfed abc h p pp pp pp pp pp pp pp pp gfed abc h gfed abc h w gfed abc gfed abc w gfed abc h p pp pp pp pp pp pp pp pp gfed abc h gfed abc h gfed abc h gfed abc h r gfed abc h w gfed abc q h gfed abc h figur know slp repres straight line trainabl weight layer sever straight line combin form convex polygon use trainabl weight layer sever polygon form arbitrari set classifi set hyperplan convex polygon set set well advantag tabl represent perceptron classifi type set number trainabl weight layer backpropag error general delta rule allow mlp train next deriv explain backpropag error learn rule abbrevi backpropag backprop bp use train multi stage perceptron semi linear activ function binari threshold function differenti function longer support matter seen fermi function hyperbol tangent arbitrarili approxim binari threshold function mean temperatur paramet larg extent follow deriv accord zel mr point procedur previous publish paul werbo wer considerabi less reader mr backpropag gradient descent procedur includ strength weak gradient descent error function err w receiv weight argument fig page assign output error dimension err w point small error even point smallest error sought mean gradient descent thus analog delta rule backpropag train weight neural network exact delta rule variabl neuron expand one trainabl weight layer sever one backpropag semilinear function monoton differenti general linear k wk h w k onmlhijk fact xrrrrrrrrrrrrrrr wh h h figur illustr posit neuron h within neural network lie layer h preced layer k subsequ layer deriv similar one delta rule general delta let us defin advanc network input individu neuron result weight sum furthermor deriv delta rule let netp etc defin alreadi familiar oi neti etc input pattern p use train let output function ident thus oi fact netp hold neuron sinc general delta rule use formula framework delta rule equat page alreadi indic general variabl everi neuron first neuron calcul obvious select arbitrari inner neuron h set k predecessor neuron k well set successor neuron inner neuron see fig therefor irrelev whether predecessor neuron alreadi input neuron perform deriv delta rule split function mean chain rule discuss deriv great detail princip similar delta rule differ alreadi mention general initi deriv error function err accord weight wk h err wk h wk h err neth z h neth wk h first factor equat h deal later text numer second factor equat includ network input weight sum includ numer immedi deriv summand sum drop apart summand contain wk h summand refer wk h ok calcul deriv output neuron k becom neth wk h p k k wk hok wk h ok promis discuss h equat split accord chain rule h err neth err oh oh neth deriv output accord network input second factor equat clear equal deriv activ function accord network input oh neth fact neth neth fact neth consid import passag analog deriv first factor equat therefor point deriv error function accord output inner neuron layer depend vector network input next follow layer reflect equat err oh err netl netl oh accord definit multi dimension chain rule immedi obtain equat err oh x err netl netl oh sum equat contain two factor discuss factor ad subsequ layer simpli calcul second factor follow equat netl oh p h h wh oh oh wh appli first factor accord definit err netl insert err oh x lwh find graphic version general includ split fig follow page reader might alreadi notic intermedi result shown frame exact intermedi result highlight way factor chang weight wk h aforement equat combin highlight intermedi result outcom want chang weight wk h wk h ok h h f act neth x lwh cours case h inner neuron otherweis would subsequ layer h err neth oh neth err oh f act neth err netl p netl oh p h h wh oh oh wh figur graphic represent equat equal sign chain rule split arrow framework backpropag deriv leav tree reflect final result general frame deriv case h output neuron alreadi discuss deriv delta rule result general delta rule call backpropag error wk h ok h h f act neth th yh h outsid f act neth p lwh h insid contrast delta rule treat differ depend whether h output inner hidden neuron h output neuron p h f act netp h tp h yp h thus train pattern p weight wk h k h chang proport accord learn rate output k predecessor neuron k gradient activ function posit network input successor neuron f act netp h differ teach input tp h output yp h successor neuron h case backpropag work two neuron layer output layer successor neuron h preced layer predecessor neuron k h inner hidden neuron p h f act netp h x p wh hold explicit mention backpropag work three layer neuron k predecessor connect chang weight wk h neuron h successor connect chang neuron lie layer follow successor neuron thus accord train pattern p weight wk h k h proport chang accord learn rate output predecessor neuron k gradient activ function posit network input successor neuron f act netp h well differ accord weight sum chang weight neuron follow h p p wh definit backpropag summar formula previous page preced page receiv follow final formula backpropag identifi p ommit reason clariti wk h ok h h f act neth th yh h outsid f act neth p lwh h insid snipe onlin variant backpropag implement method trainbackpropagationoferror within class neuralnetwork obvious backpropag initi process last weight layer direct mean teach input work backward layer layer consid preced chang weight thus teach input leav trace weight layer describ first delta rule second part backpropag general delta rule layer one go may meet requir matter research first part obvious soon see framework mathemat gimmick decad develop time work lie first second recurs part like mani groundbreak invent develop recogn plausibl invent head back boil backpropag delta rule explain delta rule special case backpropag one stage perceptron linear activ function briefli explain circumst develop delta rule backpropag order augment understand rule seen backpropag defin wk h ok h h f act neth th yh h outsid f act neth p lwh h insid sinc use one stage perceptron second part backpropag light color omit without substitut result wk h ok h h f act neth th oh furthermor use linear activ function f act light color constant general known constant combin therefor direct merg constant deriv f act constant least one lern cycl learn rate light color thus result wk h ok h ok th oh exact correspond delta rule definit select learn rate heavi influenc learn process meantim often seen chang weight case proport learn rate thus select crucial behaviour backpropag learn procedur general definit learn rate speed accuraci learn procedur alway control alway proport learn rate written valu chosen larg jump error surfac larg exampl narrow valley could simpli jump addit movement across error surfac would uncontrol thus small desir input howev cost huge often unaccept amount time experi show good learn rate valu rang select signific depend problem network train data bare possibl give practic advis instanc popular start relat larg g slowli decreas simpler problem often kept constant variat learn rate time train anoth stylist devic variabl learn rate begin larg learn rate lead good result later result inaccur learn smaller learn rate time consum result precis thus learn process learn rate need decreas one order magnitud repeat common error seem neat solut first glanc continu decreas learn rate quick happen descent learn rate larger ascent hill error function climb result simpli get stuck ascent solut rather reduc learn rate gradual mention differ layer differ learn rate farer move away output layer learn process slower backpropag learn thus good idea select larger learn rate weight layer close input layer weight layer close output layer resili backpropag extens backpropag error rais two backpropag specif properti occasion problem addit alreadi caus gradient descent one hand user backpropag choos bad learn rate hand weight output layer slower backpropag learn reason martin riedmil enhanc backpropag call version resili backpropag short rprop rb rie compar backpropag rprop without explicit declar one version superior actual deal formula let us inform compar two primari idea behind rprop consequ alreadi familiar backpropag learn rate backpropag use default learn rate select user appli entir network remain static manual chang alreadi explor disadvantag approach rprop pursu complet differ approach global learn rate first weight wi learn rate second learn rate chosen user automat set rprop third weight chang static adapt time step rprop account tempor chang correct call enabl focus learn problem increas slow learn throughout layer solv eleg way weight chang use backpropag weight chang proport gradient error function first glanc realli intuit howev incorpor everi jag featur error surfac weight chang least question whether alway use rprop take way well amount weight chang wi simpli direct correspond automat adjust learn rate thus chang weight proport gradient influenc sign gradient still know exact adapt run time let anticip result process look consider less rug error function contrast backprop weight updat step replac addit step adjust learn rate ad exact idea implement weight chang proport gradient let us first consid chang weight alreadi notic weightspecif learn rate direct serv absolut valu chang respect weight remain question sign come point gradient come play deriv backpropag deriv error function err w individu weight wi obtain gradient err w wi big differ rather multipl incorpor absolut valu gradient weight chang consid sign gradient gradient henc longer determin strength direct weight chang sign gradient err w wi posit must decreas weight wi weight reduc sign gradient negat weight need increas ad gradient exact noth happen let us creat formula colloqui descript correspond term affix show everyth happen time step might decreas clariti first glanc nevertheless import soon look anoth formula oper differ time step instead shorten gradient g err w wi definit weight chang rprop wi g g otherwis know weight chang remain question learn rate adjust final understood overal system deal remain detail like initi specif constant mani dynam adjust learn rate instead one static adjust learn rate consid associ gradient g two time step gradient pass current one sign gradient matter must ask happen sign two time step stay flip sign chang g g skip local minimum gradient henc last updat larg reduc compar previous one say search need accur mathemat term obtain new multipli old constant case know last time step someth went wrong henc addit reset weight updat weight wi time step appli shown follow formula howev sign remain one perform care increas get past shallow area error function obtain new multipli old constant greater definit adapt learn rate rprop g g g g otherwis caution impli rprop exclus design offlin gradient certain continu learn process slow lowest rate remain learn onlin one chang loos speak error function new epoch sinc base one train pattern may often well applic backpropag often even faster offlin version use frequent lack howev clear mathemat motiv exact need still miss detail use rprop practic minor issu remain unansw name larg much learn rate reinforc weaken choos weight specif learn rate initi upper lower bound max set answer question quick motiv initi valu learn rate somewher order initi weight proven good choic author rprop paper explain obvious way valu long posit without exorbit high absolut valu need dealt critic quick overridden automat adapt anyway equal uncrit max recommend without mathemat justif valu use throughout literatur one set paramet lower valu order allow cautious updat small updat step allow case set left paramet let us start valu use skip minimum know exact lie skip track analog procedur binari search target object often skip well assum middl skip track need halv learn rate canon choic select valu use learn rate shall increas caution cannot general principl binari search simpli use valu otherwis learn rate updat consist almost exclus chang direct independ particular problem valu proven protipp sinc chang multipl would rather suboptim initi promis slight chang valu signific affect rate converg fact allow set valu constant well advanc comput capabl comput one observ widespread distribut network consist big number layer deep network network crucial prefer rprop origin backpropag backprop alreadi indic learn slowli weight wich far output layer problem smaller number layer would recommend test widespread backpropag offlin onlin learn less common rprop equival snipe snipe resili backpropag support via method trainresilientbackpropag class neuralnetwork furthermor use addit improv resili propag howev dealt work getter setter differ paramet rprop backpropag often extend alter besid rprop backpropag often extend mani extens simpli implement option featur backpropag order larger scope test follow briefli describ ad momentum learn let us assum descent steep slope ski prevent us immedi stop edg slope plateau exact momentum backpropag momentum term rhw b respons fact kind moment inertia momentum ad everi step size fig next page alway ad fraction previous chang everi new chang weight pwi p pwi previous cours notat use better understand general alreadi defin concept time refer current cycl previous cycl identifi continu success formal definit momentum term figur execut gradient descent like skier cross slope would hard stop immedi edg plateau definit momentum term variat backpropag mean momentum term defin follow wi oi wi acceler plateaus avoid quasi standstil plateaus slow craggi surfac prevent oscil moreov effect inertia vari via prefactor common valu addit momentum enabl posit effect skier swing back forth sever time minimum final land minimum despit nice one dimension appear otherwis rare error leav good minima unfortun occur frequent momentum term mean optim solut accustom condit flat spot elimin prevent neuron get stuck must point hyperbol tangent well fermi function deriv outsid close proxim near result fact becom difficult move neuron away limit activ flat spot could extrem extend learn time problem dealt modifi deriv exampl ad constant g call flat spot elimin colloqui fudg interest observ success achiev use deriv defin constant fah nice exampl make use effect fast hyperbol tangent approxim anguita introduc section page outer region well approxim acceler deriv make use small constant second deriv use accord david parker second order backpropag uses second gradient second multi dimension deriv error function obtain precis estim correct wi even higher deriv rare improv estim thus less train cycl need requir much comput effort general use deriv hessian matric sinc function multidimension higher order method expect procedur reduc number learn epoch signific increas comput effort individu epoch procedur often need learn time backpropag quickpropag learn procedur fah use second deriv error propag local understand error function parabola analyt determin vertex lowest point said parabola direct jump point thus learn procedur second order procedur cours work error surfac cannot local approxim parabola certain alway possibl direct say whether case weight decay punish larg weight weight decay accord paul werbo wer modif extend error term punish larg weight error weight decay errwd increas proport actual error proport squar weight result network keep weight small learn errwd err x w w w z punish approach inspir natur synapt weight cannot becom infinit strong well addit due small weight error function often show weaker fluctuat allow easier control learn prefactor result simpl pragmat factor control strength punish valu often use cut network prune optim brain damag execut weight decay long enough notic neuron input layer successor weight close remov neuron henc lose neuron weight therebi reduc possibl network memor procedur call prune method detect delet unnecessari weight neuron refer optim brain damag lcds describ briefli mean error output neuron compos two compet term one term usual consid differ output teach input one tri press weight toward weight strong need minim error first term win case second term win neuron zero weight prune mani variat backprop whole book subject sinc aim offer overview neural network mention variat motiv read extens obvious cannot appli feedforward network backpropag learn procedur gotten know backpropag feedforward topolog learn build neural network cours imposs fulli communic experi framework work obtain least knowledg advis deal exemplari problem get start initi configur multilay perceptron discuss backpropag error learn procedur know train exist network would use consid implement network number layer two three may often job use let us begin trivial circumst network one layer input neuron one layer output neuron result least two layer addit need alreadi learn examin linear separ least one hidden layer neuron problem linear separ seen like possibl alreadi mention mathemat prove mlp one hidden neuron layer alreadi capabl approxim arbitrari function accuraci necessari discuss represent problem mean perceptron learnabl represent mean perceptron principl realiz map learnabl mean abl teach respect experi show two hidden neuron layer three trainabl weight layer use solv problem sinc mani problem repres hidden layer difficult learn one keep mind addit layer generat addit sub minima error function get stuck thing consid promis way tri one hidden layer first fail retri two layer fail one consid layer howev given increas calcul power current comput deep network lot layer use success note indic number neuron hidden layer mention hypothet possibl number neuron test number neuron apart input output layer number input output neuron alreadi defin problem statement princip correspond number free paramet problem repres sinc alreadi discuss network capac respect memor imprecis problem represent clear goal free paramet possibl mani necessari know standard solut question mani neuron use thus use approach initi train neuron repeat train new network neuron result signific improv particular general perform affect bottom approach select activ function anoth import paramet way inform process neural network select activ function activ function input neuron fix ident function sinc process inform first question ask whether actual use activ function hidden layer ouput layer one prevent us choos differ function general activ function hidden neuron well output neuron respect task function approxim found reason use hyperbol tangent left part fig next page activ function hidden neuron linear activ function use output latter absolut necessari generat limit output interval contrari input layer use linear activ function well output layer still process inform threshold valu howev linear activ function output caus huge learn step jump good minima error surfac avoid set learn rate small valu output layer unlimit output interv essenti pattern recognit task hyperbol tangent use case output interv bit larger unlik general pattern recognit understood special case function approxim discret output possibl tanh x x hyperbol tangent f x x fermi function temperatur paramet figur remind illustr hyperbol tangent left fermi function right fermi function expand temperatur paramet origin fermi function therebi repres dark color temperatur paramet modifi fermi function order ascend steep hyperbol tangent fermi function right part fig difficult learn someth far threshold valu result close howev lot freedom given select activ function general disadvantag sigmoid function fact hard learn someth valu far thei threshold valu unless network modifi weight initi small random chosen valu initi weight trivial one might think simpli initi chang weight initi valu chang equal train simpl solut problem call symmetri break initi weight small random valu rang random valu could interv includ valu close random initi nice side effect chanc averag network input close valu hit activ function region greatest deriv allow strong learn impuls right start learn snipe snipe weight initi random synaps initi want maximum absolut weight valu synaps initi random set neuralnetworkdescriptor use method setsynapseinitialrang encod problem relat problem encod problem classic among multilay perceptron test train problem mlp input layer eight neuron output layer eight neuron one hidden layer three neuron thus network repres function b b train task input valu neuron ij lead output valu neuron one neuron activ result train sampl analysi train network see network hidden neuron repres kind binari encod map possibl assum train time epoch thus network machin input first encod afterward decod analog train encod problem possibl improv effici procedur could exampl encod network yes even possibl sinc network depend binari encod thus network suffici problem encod network far difficult understand fig next page train network requir lot time snipe static method getencodersamplelesson class trainingsamplelesson allow creat simpl train sampl lesson arbitrari dimension encod problem like network howev work sinc possibl output one neuron compens anoth one essenti one hidden neuron certain compensatori neuron exercis exercis fig page show small network boolean function write tabl comput paramet neural network g network input activ etc perform calcul four possibl input network write valu variabl input xor network fig page exercis figur illustr function network encod mark point repres vector inner neuron activ associ sampl see possibl find inner activ format point separ rest point straight line illustr show exemplari separ one point list boolean function b b linear separ character exact list linear separ character exact exercis simpl network shall train one singl pattern mean backpropag error verifi error err errp converg valu error curv look like let pattern p defin p p p random init weight interv exercis one stage perceptron two input neuron bias neuron binari threshold function activ function divid two dimension space two region mean straight line g analyt calcul set weight valu perceptron follow set p pattern form p p correct classifi p exercis calcul comprehens way one vector w chang weight mean backpropag error procedur let mlp bias neuron given let pattern defin p p p weight target initi valu weight weight initi valu conspicu chang chapter radial basi function rbf network approxim function stretch compress gaussian bell sum spatial shift descript function learn process comparison multilay perceptron accord poggio girosi pg radial basi function network rbf network paradigm neural network develop consider later perceptron like perceptron rbf network built layer case exact three layer one singl layer hidden neuron like perceptron network feedforward structur layer complet link input layer particip inform process rbf network like mlps univers function approxim despit thing common differ rbf network perceptron differ lie inform process comput rule within neuron outsid input layer moment defin far unknown type neuron compon structur rbf network initi discuss colloqui defin concept concern rbf network output neuron rbf network output neuron contain ident activ function one weight sum propag function thus littl ad input valu return sum hidden neuron call rbf neuron well layer locat refer rbf layer propag function hidden neuron calcul norm repres distanc input network call posit neuron center insert radial activ function calcul output activ neuron definit rbf input neuron definit represent ident definit page input neuron definit center rbf neuron center ch rbf neuron h point input space rbf neuron locat general closer input vector center vector rbf neuron higher activ definit rbf neuron call rbf neuron h propag function fprop determin distanc center ch neuron input vector distanc repres network input network input sent radial basi function fact return activ output neuron rbf neuron repres symbol wvutpqr x gau definit rbf output neuron rbf output neuron use weight sum propag function fprop ident activ function fact repres symbol onmlhijk definit rbf network rbf network exact three layer follow order input layer consist input neuron hidden layer call rbf layer consist rbf neuron output layer consist rbf output neuron layer complet link follow one shortcut exist fig next page feedforward topolog connect input layer rbf layer unweight transmit input connect rbf layer output layer weight origin definit rbf network refer output neuron analog perceptron appar definit general bias neuron use rbf network set input neuron shall repres set hidden neuron h set output neuron therefor inner neuron call radial basi neuron definit follow direct input vector distanc center neuron produc output valu fig face page gfed abc gfed abc v wvutpqr x gau wvutpqr x gau wvutpqr x gau wvutpqr x gau v wvutpqr x gau v h h h h onmlhijk onmlhijk onmlhijk figur exemplari rbf network two input neuron five hidden neuron three output neuron connect hidden neuron weight transmit input right illustr find name neuron coincid name mlp neuron input neuron call hidden neuron call h output neuron call associ set refer h figur let ch center rbf neuron h activ function facth radial symmetr around ch h r r gaussian gaussian x h r figur two individu one two dimension gaussian bell case hold center gaussian bell lie coordin origin distanc r center simpli calcul accord pythagorean theorem r p x inform process rbf network question realiz network purpos let us go rbf network top bottom rbf network receiv input mean unweight connect input vector sent norm result scalar scalar way posit due norm process radial basi function exampl gaussian bell fig output valu differ neuron rbf layer differ gaussian bell ad within third layer basic relat whole input space gaussian bell ad suppos second third fourth rbf neuron therefor four differ locat center neuron measur anoth distanc input center facto provid differ valu even gaussian bell sinc valu final simpli accumul output layer one easili see surfac shape drag compress remov gaussian bell subsequ accumul paramet superposit gaussian bell weight connect rbf layer output layer x figur four differ gaussian bell one dimension space generat mean rbf neuron ad output neuron rbf network gaussian bell differ height width posit center locat width see two dimension exampl fig follow page furthermor network architectur offer possibl freeli defin train height width gaussian bell due network paradigm becom even versatil get know method approch later inform process rbf neuron rbf neuron process inform use norm radial basi function first let us take exampl simpl rbf network appar receiv one dimension output repres function fig addit network includ center four inner neuron h h h therefor gaussian bell final ad within output neuron network possess four valu influenc width gaussian bell contrari height gaussian bell influenc subsequ weight sinc individu output valu bell multipli weight gaussian x h r gaussian x h r gaussian x h r gaussian x h r wvutpqr x gau wvutpqr x gau wvutpqr x gau wvutpqr x gau v onmlhijk sum gaussian x figur four differ gaussian bell two dimension space generat mean rbf neuron ad output neuron rbf network r p x appli distanc height w width center x w w w w sinc use norm calcul distanc input vector center neuron h differ choic often euclidian norm chosen calcul distanc rh x ch sx xi ch rememb input vector refer x index run input neuron therebi input vector compon neuron center compon see euclidean distanc generat squar differ vector compon add extract root sum two dimension space correspond pythagorean theorem definit norm direct follow distanc posit strict speak henc use posit part activ function way activ function gaussian bell possibl normal function monoton decreas interv chosen know distanc rh input vector x center ch rbf neuron h distanc pass activ function use alreadi mention gaussian bell fact rh r h h obvious center ch width h seen part activ function fact henc activ function refer fact simultan one solut would number activ function like fact fact fact h h set hidden neuron result explan would confus simpli use name fact activ function regard variabl defin individu neuron direct includ activ function reader certain notic literatur gaussian bell often normal multipl factor howev avoid factor multipli anyway subsequ weight consecut multipl first normal factor connect weight would yield differ factor need factor especi purpos integr gaussian bell must alway therefor simpli leav analyt thought prior train output rbf output neuron result combin function rbf neuron x h h wh fact x ch suppos similar multilay perceptron set p contain p train sampl p obtain p function form x h h wh fact p ch one function train sampl cours effort aim let output train pattern p converg correspond teach input weight simpli comput solut system equat thus p equat let us assum width k center ck train sampl p includ teach input given look weight wh h weight one output neuron thus problem seen system equat sinc thing chang moment weight demand distinct case concern number train sampl p number rbf neuron h p h number rbf neuron equal number pattern p h equat reduc matrix multipl g g g g vector teach input train sampl p h matrix output h rbf neuron p sampl rememb p h matrix squar therefor attempt invert g vector desir weight unit matrix size g mathemat speak simpli calcul weight case p h exact one rbf neuron avail train sampl mean network exact meet p exist node calcul weight perform precis interpol calcul equat certain need rbf network therefor proceed next case exact interpol must mistaken memor abil mention mlps first talk train rbf network moment second could advantag us might fact intend network exact interpol node p h system equat determin rbf neuron train sampl p h certain case normal occur often case huge varieti solut need detail select one set weight mani obvious possibl one p h interest discuss case signifi cant train sampl rbf neuron mean p h thus use general capabl neural network train sampl rbf neuron cannot assum everi train sampl exact hit cannot exact hit point therefor cannot interpol aforement ideal case p h must tri find function approxim train set p close possibl mlp tri reduc sum squar error minimum continu calcul case p h solv system equat find solut matrix multipl g problem time cannot invert p h matrix squar matrix p h true use moor penros pseudo invers defin mt mt although moor penros pseudo invers invers matrix use similar case get equat similar case p h g g g g anoth reason use moor penros pseudo invers fact minim squar error goal estim vector g equat correspond gauss markov model known statist use minim squar error aforement equat follow one pleas mistak mt transpos matrix vector teach input general sever output trivial quit comput expens found mathemat exact way direct calcul weight happen sever output neuron usual set output neuron case alreadi indic chang much addit output neuron set weight chang rbf layer thus rbf network easi given realiz lot output neuron sinc calcul individu vector weight g everi new output neuron wherea matrix general requir lot comput effort alway stay quit inexpens least concern comput complex add output neuron particular true invert go detail reason circumst applic easili found literatur linear algebra comput effort accuraci realist problem normal appli consider train sampl rbf neuron p h without difficulti use train sampl like theoret could find term mathemat correct solut blackboard long time calcul often seem imprecis time consum matrix invers requir lot comput effort furthermor moor penros pseudo invers spite numer stabil guarante output vector correspond teach vector extens comput prone mani inaccuraci even though calcul mathemat correct comput provid us nonetheless good approxim pseudo invers matric mean get approxim correct weight mayb lot accumul numer error therefor approxim mayb rough even unrecogniz desir output enough comput power analyt determin weight vector use nevertheless initi valu learn process lead us real train method otherwis would bore combin equat system gradient strategi use train analog mlp perform gradient descent find suitabl weight mean alreadi well known delta rule backpropag unnecessari sinc train one singl weight layer requir less comput time know delta rule wh oh insert follow wh fact p ch explicit mention popular divid train two phase analyt comput set weight refin train delta rule still question whether learn offlin onlin answer similar answer multilay perceptron initi one often train onlin faster movement across error surfac approxim solut error accumul precis approxim one train offlin third learn phase howev similar mlps success use mani method alreadi indic rbf network weight hidden output layer optim let us take look possibl vari alway trivial determin center width rbf neuron obvious approxim accuraci rbf network increas adapt width posit gaussian bell input space problem need approxim sever method deal center width gaussian bell fix select center width select fix manner regardless train sampl assum condit fix select center width select fix previous knowledg function approxim compli adapt learn process definit eleg variant certain challeng one realize approach discuss chapter found connect anoth network topolog section fix select case goal cover input space even possibl width distanc center select gaussian bell overlap approx one third fig next page closer bell set precis time consum whole thing becom appar gaussian bell mathemat infinit wide therefor ask reader apolog sloppi formul figur exampl even coverag two dimension input space appli radial basi function may seem ineleg field function approxim cannot avoid even coverag useless function approxim precis repres posit posit return valu howev high input dimens requir great mani rbf neuron increas comput effort exponenti dimens respons fact six ten dimension problem rbf network alreadi call high dimension mlp exampl caus problem condit fix select suppos train sampl even distribut across input space seem obvious arrang center sigma rbf neuron mean pattern distribut train pattern analyz statist techniqu cluster analysi determin whether statist factor accord distribut center sigma fig follow page trivial altern would set h center posit random select set pattern method would allow everi train pattern p figur exampl uneven coverag two dimension input space previous knowledg appli radial basi function direct center neuron fig next page yet eleg good solut time issu general method width fix select reason believ set train sampl cluster use cluster method determin differ method determin cluster arbitrarili dimension set point introduc excursus one neural cluster method call rolf section self organ map use connect determin posit rbf neuron section use rolf one receiv indic use radii rbf neuron learn vector quantis chapter provid good result method noth rbf network use generat previous knowledg therefor discuss chapter independ indic chapter anoth approach use approv method could slight move posit center observ error function err chang gradient descent figur exampl uneven coverag two dimension input space appli radial basi function width fix select center neuron random distribut throughout train pattern distribut certain lead slight unrepres result seen singl data point left alreadi known mlps similar manner could look error depend valu analog deriv backpropag deriv err hch h err hch ch sinc deriv term correspond deriv backpropag discuss experi show convinc result obtain regard error behav depend center sigma even mathemat claim method promis gradient descent alreadi know lead problem craggi error surfac crucial point natur rbf network generat craggi error surfac consider chang signific chang appear error function grow rbf network automat adjust neuron densiti grow rbf network number h rbf neuron constant certain number h neuron well center ch width h previous select g mean cluster method extend reduc follow text simpl mechan sketch inform refer fri neuron ad place larg error valu generat initi configur vector weight g analyt calcul specif error errp concern set p train sampl calcul maximum specif error max p errp sought extens network simpl replac maximum error new rbf neuron cours exercis care small neuron influenc distanc short larg alreadi exisit neuron consider influenc new neuron overlap gaussian bell obvious adjust alreadi exist rbf neuron ad new neuron put simpli adjust made move center neuron away new neuron reduc width bit current output vector network compar teach input weight vector g improv mean train subsequ new neuron insert necessari method particular suit function approxim limit number neuron mandatori see network grow infinitum happen fast thus use previous defin maximum number neuron h max less import neuron delet lead question whether possibl continu learn limit h max reach answer would stop learn look unimport neuron delet neuron exampl unimport network anoth neuron similar function often occur two gaussian bell exact overlap posit instanc one singl neuron higher gaussian bell would appropri develop autom procedur order find less relev neuron high problem depend leav programm rbf network multilay perceptron alreadi becom acquaint extensivley discuss two network paradigm similar problem therefor compar two paradigm look advantag disadvantag compar rbf network multilay perceptron compar multilay perceptron rbf network respect differ aspect input dimens must care rbf network high dimension function space sinc network could quick requir huge memori storag comput effort multilay perceptron would caus less problem number neuon grow exponenti input dimens center select howev select center rbf network despit introduc approach still major problem pleas use previous knowledg appli problem occur mlp output dimens advantag rbf network train much influenc output dimens network high mlp learn procedur backpropag therebi time consum extrapol advantag well disadvantag rbf network lack extrapol capabl rbf network return result far away center rbf layer one hand extrapol unlik mlp cannot use extrapol wherebi could never know extrapol valu mlp reason experi show mlps suitabl matter hand unlik mlp network capabl use tell us know could advantag lesion toler output mlp import weight neuron miss worsen littl total weight neuron miss rbf network larg part output remain practic uninfluenc one part output heavili affect gaussian bell direct miss thus choos strong local error lesion weak global error spread mlp advantag sinc rbf network use consider less often alway understood profession least far lowdimension input space concern mlps seem consider longer tradit work good take effort read page work rbf network exercis exercis h rbf network fix width center neuron approxim target function p train sampl form p function given let p h true weight analyt determin mean moor penros pseudo invers indic run time behavior regard p precis possibl note method matrix multipl matrix invers effici canon method better estim recommend look method complex addit complex calcul pleas indic use method togeth complex chapter recurr perceptron like network thought network intern state general recurr network network capabl influenc mean recurr g includ network output follow comput step mani type recurr network near arbitrari form near refer recurr neural network result paradigm introduc use name recurr multilay perceptron appar recurr network capabl comput ordinari mlp recurr weight set recurr network reduc ordinari mlp addit recurr generat differ network intern state differ input produc differ output context network state recurr network great dynam mathemat difficult conceiv discuss extens aim chapter briefli discuss recurr structur network intern state generat thus briefli introduc two paradigm recurr network afterward rough outlin train recurr network input x constant time may lead differ result one hand network could converg could transform fix state time return fix output valu hand could never converg least long time later longer recogn consequ constant chang figur roessler attractor network converg exampl possibl check period attractor fig return expect complet varieti dynam system reason particular refer literatur concern dynam system discuss could reveal happen input recurr network chang chapter relat paradigm recurr network accord jordan elman introduc jordan network jordan network jor multilay perceptron set k call context neuron k k k k one context neuron output neuron fig next page principl context neuron memor output gfed abc gfed abc gfed abc k x gfed abc k v gfed abc h gfed abc h gfed abc h gfed abc bc oo gfed abc oo figur illustr jordan network network output buffer context neuron next time step enter network togeth new input process next time step therefor weight connect output neuron one context neuron store valu return actual network mean complet link context neuron input layer origini definit jordan network context neuron recurr via connect weight applic omit recurr sinc jordan network alreadi dynam difficult analyz even without addit recurr definit context neuron context neuron k receiv output valu anoth neuron time reenter network time definit jordan network jordan network multilay perceptron one context neuron output neuron set context neuron call k context neuron complet link toward input layer network gfed abc gfed abc gfed abc h gfed abc h gfed abc h onmlhijk kh v z onmlhijk kh w onmlhijk kh v gfed abc gfed abc onmlhijk k w onmlhijk k v figur illustr elman network entir inform process part network exist way twice output neuron except output input neuron buffer reenter associ layer reason clariti name context neuron basi model actual network mandatori elman network elman network variat jordan network elm context neuron one layer context neuron inform process neuron layer fig thus output hidden neuron output neuron led associ context layer exact one context neuron neuron reenter complet neuron layer next time step complet link way back complet inform process part mlp exist second time context version consider increas dynam state varieti compar jordan network elman network often advantag act purpos sinc everi layer access context definit elman network elman network mlp one context neuron inform process neuron set context neuron call k rememb input layer process inform mean exist one context layer inform process neuron layer exact number context neuron everi neuron weight connect exact one context neuron context layer complet link toward origin layer interest take look train recurr network sinc instanc ordinari backpropag error cannot work recurr network style follow part rather inform mean use formal definit train recurr network order explain train comprehens possibl agre simplif affect learn principl train let us assum begin context neuron initi input sinc otherwis would undefin input simplif realiti furthermor use jordan network without hidden neuron layer train attempt output neuron direct provid input approach strong simplif general complic network use chang learn principl unfold time rememb actual learn procedur mlps backpropag error backpropag delta valu case recurr network delta valu would backpropag cyclic network make train difficult one hand cannot know mani generat delta valu weight select train valu use hand cannot definit know learn stop advantag recurr network great state dynam within network disadvantag recurr network dynam grant train therefor make difficult one learn approach would attempt unfold tempor state network fig page recurs delet put similar network context neuron context neuron manner speak output neuron attach network general spoken backtrack recurr place earlier instanc neuron network thus creat larger forward orient network without recurr enabl train recurr network train strategi develop recurr one input enter teach input everi copi input neuron done discret number time step train paradigm call unfold time mp unfold train mean backpropag error possibl obvious one weight wi sever chang valu wi receiv treat differ accumul averag etc simpl accumul could possibl result enorm chang weight chang sign henc averag underestim could introduc discount factor weaken influenc wi past unfold time particular use receiv impress closer past import network one away reason backpropag littl influenc layer farther away output rememb farther output layer smaller influenc backpropag disadvantag train unfold network take long time sinc larg number layer could possibl produc problem longer neglig limit comput accuraci ordinari comput exhaust fast mani nest comput farther output layer smaller influenc backpropag limit reach furthermor sever level context neuron procedur could produc larg network train teacher forc procedur equival teacher forc open loop learn detach recurr learn process simpli pretend recurr exist appli teach input context neuron train backpropag becom possibl disadvantag elman network teach input output neuron given gfed abc gfed abc p pp pp pp pp pp pp pp pp gfed abc gfed abc k w gfed abc k w gfed abc bc oo gfed abc oo p pp pp pp pp pp pp pp w w v w gfed abc gfed abc p pp pp pp pp pp pp pp pp gfed abc gfed abc k w gfed abc k w gfed abc gfed abc figur illustr unfold time small exemplari recurr mlp top recurr mlp bottom unfold network reason clariti ad name lowest part unfold network dot arrow lead network mark input dot arrow lead network mark output network copi repres time step network recent time step bottom recurr backpropag anoth popular procedur without limit time horizon recurr backpropag use method differenti calculus solv problem pin train evolut due alreadi long last train time evolutionari algorithm prove valu especi recurr network one reason unrestrict respect recurr advantag mutat mechan chosen suitabl exampl neuron weight adjust network topolog optim cours result learn necessarili jordan elman network ordinari mlps howev evolutionari strategi less popular sinc certain need lot time direct learn procedur backpropag chapter hopfield network magnet field particl appli forc particl particl adjust movement energet favor way natur mechan copi adjust noisi input order match real model anoth supervis learn exampl wide rang neural network develop john hopfield call hopfield network hop hopfield physic motiv network contribut lot renaiss neural network hopfield network inspir particl magnet field idea hopfield network origin behavior particl magnet field everi particl communic mean magnet forc everi particl complet link particl tri reach energet favor state minimum energi function neuron state known activ thus particl neuron rotat therebi encourag continu rotat manner speak neural network cloud particl base fact particl automat detect minima energi function hopfield idea use spin particl process inform let particl search minima arbitrari function even use two spin binari activ recogn develop hopfield network show consider dynam oo oo figur illustr exemplari hopfield network arrow mark binari spin due complet link neuron layer cannot separ mean hopfield network simpli includ set neuron hopfield network neuron influenc symmetr briefli speak hopfield network consist set k complet link neuron binari activ sinc use two spin weight symmetr individu neuron without neuron direct connect fig thus state k neuron two possibl state describ string x k complet link provid full squar matrix weight neuron mean weight discuss follow furthermor soon recogn accord rule neuron spin chang state addit complet link lead fact know input output hidden neuron thus think input someth k neuron definit hopfield network hopfield network consist set k complet link neuron without direct recurr activ function neuron binari threshold function output definit state hopfield network state network consist activ state neuron thus state network understood binari string z k input output hopfield network repres neuron state learn network set k particl state automat look minimum input pattern hopfield network exact state binari string x k initi neuron network look minimum taken previous defin input train sampl energi surfac know minimum found simpl network stop proven hopfield network symmetr weight matrix zero diagon alway converg cg point stand still output binari string k name state string network found minimum let us take closer look content weight matrix rule state chang neuron definit input output hopfield network input hopfield network binari string x k initi state network converg network output binari string k generat new network state signific weight alreadi said neuron chang state direct vice versa spin occur depend current state neuron associ weight thus weight capabl control complet chang network weight posit negat colloqui speak weight wi two neuron follow hold wi posit tri forc two neuron becom equal larger harder network tri neuron state neuron state high posit weight advis two neuron energet favor equal wi negat behavior analogu urg differ neuron state would tri urg neuron state zero weight lead two involv neuron influenc f x x heavisid function figur illustr binari threshold function weight whole appar take way current state network toward next minimum energi function discuss neuron follow way neuron chang state accord influenc neuron network train initi start state chang state xk individu neuron k occur accord scheme xk fact x k wj k xj time step function fact general binari threshold function fig threshold colloqui speak neuron k calcul sum wj k xj indic strong direct neuron k forc neuron thus new state network time result state network previous time sum direct neuron k push depend sign sum neuron take state anoth differ hopfield network alreadi known network topolog asynchron updat neuron k random chosen everi time recalcul activ thus new activ previous chang neuron immedi influenc network one time step indic chang singl neuron regardless aforement random select neuron hopfield network often much easier implement neuron simpli process one activ recalcul chang occur definit chang state hopfield network chang state neuron occur asynchron neuron updat random chosen new state generat mean rule xk fact x wj k xj know weight influenc chang state neuron forc entir network toward minimum question teach weight forc network toward certain minimum weight matrix generat direct train pattern aim generat minima mention energi surfac input network converg mani network paradigm use set p train pattern p k repres minima energi surfac unlik mani network paradigm look minima unknown error function defin minima function purpos network shall automat take closest minimum input present seem unusu understand whole purpos later rough speak train hopfield network done train train pattern exact use rule describ follow singl shot learn pi pj state neuron p p wi x p p pi pj result weight matrix w colloqui speak initi network mean train pattern process weight wi one anoth weight verifi neuron state state vari first case add weight second case add repeat train pattern p p final valu weight wi high correspond mani train pattern colloqui speak high valu tell neuron often energet favor hold state appli negat weight due train store certain fix number pattern p weight matrix input x network converg store pattern closest input p unfortun number maximum storabl reconstruct pattern p limit p max k turn appli orthogon pattern shown precis time consum mathemat analys specifi pattern enter alreadi store inform destroy definit learn rule hopfield network individu element weight matrix w defin singl process learn rule wi x p p pi pj diagon matrix cover zero p max k train sampl train time maintain function know function hopfield network noth practic use autoassoci tradit applic hopfield network like mention call autoassoci autoassoci exact show aforement behavior first known pattern p enter exact known pattern return thus p p associ map second practic use work input close pattern p p afterward autoassoci case stabl state name state p set pattern p consist exampl letter charact form pixel network abl correct recogn deform noisi letter high probabl fig follow page primari field applic hopfield network pattern recognit pattern complet zip code recognit letter eighti soon hopfield network replac system field applic exampl ocr system field letter recognit today hopfield network virtual longer use becom establish practic heteroassoci analog neural data storag far introduc hopfield network converg arbitrari input closest minimum static energi surfac anoth variant dynam energi surfac appear energi surfac depend current state receiv heteroassoci instead autoassoci heteroassoci p p longer true rather h p q mean pattern map onto anoth one h heteroassoci map heteroassoci achiev mean asymmetr weight matrix v figur illustr converg exemplari hopfield network pictur binari pixel hopfield network pixel correspond one neuron upper illustr show train sampl lower show converg heavili noisi correspond train sampl heteroassoci connect seri form h p q h q r h r h z p provok fast cycl state p q r z p wherebi singl pattern never complet accept pattern entir complet heteroassoci alreadi tri generat successor pattern addit network would never stop sinc reach last state z would proceed first state p generat heteroassoci matrix generat matrix v mean element v similar autoassoci matrix p transit train sampl transit q train sampl generat p x p q p p q piqj diagon matrix fill zero neuron state alway adapt oper sever transit introduc matrix simpl addit wherebi said limit exist definit learn rule heteroassoci matrix two train sampl p predecessor q successor heteroassoci transit weight heteroassoci matrix v result learn rule x p q p p q piqj sever heteroassoci introduc network simpl addit stabil heteroassoci alreadi mention problem pattern complet generat next pattern alreadi begin generat previous pattern finish problem avoid influenc network mean heteroassoci matrix v alreadi known autoassoci matrix w addit neuron adapt rule chang compet term generat one term autoassoci exist pattern one term tri convert pattern successor associ rule provok network stabil pattern remain goe next pattern xi fact x k wi jxj z autoassoci x k k kxk z heteroassoci valu caus descript speak influenc matrix v delay sinc refer network version behind result chang state individu state stabl short set exampl twenti step asymmetr weight matrix realiz chang network twenti step later initi work autoassoci matrix sinc still perceiv predecessor pattern current one work biolog motiv heterassoci biolog point view transit stabl state stabl state high motiv least begin nineti assum hopfield model achiev approxim state dynam brain realiz much mean state chain would ask dear reader recit alphabet general manag better pleas tri immedi answer follow question letter alphabet follow letter p f x x fermi function temperatur paramet figur alreadi known fermi function differ temperatur paramet variat anoth exampl phenomenon one cannot rememb situat place one memor last time perfect known one return place forgotten situat often come back mind continu hopfield network far discuss hopfield network binari activ hopfield describ version network continu activ hop cover least briefli continu hopfield network activ longer calcul binari threshold function fermi function temperatur paramet fig network stabl symmetr weight matric zero diagon hopfield state continu hopfield network appli find accept solut np hard travel salesman problem ht accord verif trial zel statement kept today faster algorithm handl problem therefor hopfield network longer use exercis exercis indic storag requir hopfield network k neuron weight wi shall store integ possibl limit valu rang weight order save storag space exercis comput weight wi hopfield network use train set p chapter learn vector quantize learn vector quantize learn procedur aim repres vector train set divid predefin class well possibl use repres vector manag vector unkown could easili assign one class slowli part ii text near therefor write last chapter part smooth transit next one chapter learn vector quantize abbrevi lvq koh describ teuvo kohonen character relat self organ featur map som describ next chapter alreadi belong part iii text sinc som learn unsupervis thus explor lvq bid farewel supervis learn previous announc differ variat lvq mention exact repres goal chapter rather analyz under principl quantize order explor learn vector quantize first get clearer pictur quantize refer discret everybodi know sequenc discret number contain natur number discret mean sequenc consist separ element interconnect element exampl exact number natur number includ exampl number hand sequenc real number r instanc continu matter close two select number alway number quantize mean continu space divid discret section delet exampl decim place real number could assign natur number obvious number front comma would assign natur number would kind repres real number within interv must note sequenc irregular quantiz instanc timelin week could quantiz work day weekend special case quantize digit case digit alway talk regular quantize continu space number system respect certain basi enter exampl number comput number digit binari system basi definit quantize separ continu space discret section definit digit regular quantize lvq divid input space separ area almost possibl describ mean name lvq enabl us set repres use divid input space class reflect input space well possibl fig face page thus element input space assign vector repres class set repres repres entir input space precis possibl vector call codebook vector codebook vector repres exact input space vector lie closest divid input space said discret area emphas know advanc mani class train sampl belong class furthermor import class must disjoint mean may overlap figur bexampl quantize two dimension input space dthe line repres class limit mark codebook vector separ data class interest mani problem use explor characterist repres instead possibl huge set vector less time consum suffici precis use codebook vector nearest one winner use prepar set codebook vector simpl input vector class associ easili decid consid codebook vector closest codebook vector build voronoi diagram set sinc codebook vector clear associ class input vector associ class adjust codebook vector alreadi indic lvq supervis learn procedur thus teach input tell learn procedur whether classif input pattern right wrong word know advanc number class repres number codebook vector rough speak aim learn procedur train sampl use caus previous defin number random initi codebook vector reflect train data precis possibl procedur learn learn work accord simpl scheme sinc learn supervis set p p train sampl addit alreadi know class predefin set class codebook vector clear assign class thus say set class contain mani codebook vector lead structur train sampl form p therefor contain train input vector p class affili class affili hold mean clear assign train sampl class codebook vector intuit could say learn learn procedur calcul averag class member place codebook vector see soon learn procedur lot briefli discuss step fundament lvq learn procedur initi place set codebook vector random posit input space train sampl train sampl p train set p select present distanc measur measur distanc p codebook vector input p winner closest codebook vector win one p learn process learn process take place accord rule h p p break alreadi seen first factor time depend learn rate allow us differenti larg learn step fine tune last factor p obvious direct toward codebook vector move function h p core rule implement distinct case assign correct winner vector codebook vector class includ p case function provid posit valu codebook vector move toward p assign wrong winner vector repres class includ p therefor move away p see definit function h precis enough good reason lvq divid differ nuanc depend exact h learn rate defin call lvq lvq lvq olvq etc differ instanc strength codebook vector movement base principl describ announc discuss therefor give formal definit regard aforement learn rule lvq connect neural network spite learn process question lvq neural network codebook vector understood neuron fix posit within input space similar rbf network addit natur often occur group one neuron may fire winner neuron codebook vector return inhibit neuron decid place brief chapter learn vector quantize approach continu follow chapter self organ map classifi input mean neuron distribut throughout input space time know input belong class let us take look unsupervis learn network exercis exercis indic quantize equal distribut vector h h five dimension unit cube h one class part iii unsupervis learn network paradigm chapter self organ featur map paradigm unsupervis learn neural network map input space fix topolog thus independ look simililar function learn procedur variat neural gas take look concept biolog neural network mention introduct one question aris brain store recal impress receiv everi day let point brain train sampl therefor desir output alreadi consid subject realiz output sens brain respond extern input chang state speak output base principl explor question biolog neural network organ teuvo kohonen develop eighti self organ featur map koh koh short refer self organ map som paradigm neural network output state network learn complet unsupervis without teacher unlik network paradigm alreadi got know som unnecessari ask neuron calcul ask neuron activ moment biolog motiv biolog neuron connect certain muscl less interest know strong certain muscl contract muscl activ word interest exact output neuron know neuron provid output thus som consider relat biolog exampl feedforward network increas use calcul structur self organ map typic som like brain task map high dimension input dimens onto area low dimension grid cell g dimens draw map high dimension space speak generat map simpli obtain arbitrari mani point input space input point tri cover good possibl posit point appear neuron particular mean everi neuron assign certain posit input space first fact seem bit confus recommend briefli reflect two space som work dimension input space g dimension grid neuron lie indic neighborhood relationship neuron therefor network topolog one dimension grid neuron could instanc like pearl string everi neuron would exact two neighbor except two neuron two dimension grid could squar array neuron fig next page anoth possibl array two dimension space would kind honeycomb shape irregular topolog possibl often topolgi dimens consider neighborhood relationship would possibl due lack visual capabl employ often even g true two space equal distinguish special case dimens initi briefli formal regard function self organ map make clear mean exampl definit neuron similar neuron rbf network neuron k occupi fix posit ck center input space definit self organ map self organ map set k neuron input vector enter exact neuron k k activ closest input pattern input space dimens input space refer definit topolog neuron interconnect neighborhood relationship neighborhood relationship call topolog train figur exampl topolog self organ map see one dimension topolog two dimension one high influenc topolog defin topolog function h k winner neuron k neuron adapt discuss later timestep dimens topolog refer g som alway activ neuron least distanc input pattern like mani neural network train use let us regard simpl function complet self organ map train sinc mani analog train function consist follow step input arbitrari valu p input space r calcul distanc everi neuron k p mean norm calcul p ck learn soon winner neuron one neuron becom activ name neuron shortest calcul distanc input neuron remain inact paradigm activ call winner take scheme output expect due input show neuron becom activ mani literatur citat descript som formal often input layer describ complet link toward layer input layer neuron forward input layer layer later link winner neuron establish inhibit neuron think explan descript therefor tri provid clearer descript network structur question neuron activ input answer given network train train train make topolog cover input space train near straightforward function describ basic structur five step partial correspond function initi network start random neuron center ck r input space creat input pattern stimulus point p select input space r stimulus enter network distanc measur distanc p ck determin everi neuron k network winner take winner neuron determin smallest distanc p fulfil condit p p ck k see sever winner neuron one select adapt center neuron center move within input space accord rule ck h k p ck valu ck simpli ad exist center last factor show chang posit neuron k proport distanc input pattern p usual time depend learn rate mention network topolog exert influenc mean function h k discuss follow definit learn rule train present input pattern determin associ winner neuron winner neuron neighbor neuron defin topolog function adapt center accord rule ck h k p ck ck ck ck topolog function defin learn neuron influenc neighbor topolog function h defin input space grid repres neighborhood relationship neuron topolog network time depend often explain paramet paramet k index run neuron paramet index winner neuron principl function shall take larg valu k neighbor winner neuron even winner neuron small valu smore precis definit topolog function must unimod must exact one maximum maximum must next winner neuron distanc certain addit time depend enabl us exampl reduc neighborhood cours time note mani sourc rule written h p ck wrong lead reader believ h constant problem easili solv omit multipl dot k k oo x q qq qq qq q qq qq q figur exampl distanc one dimension topolog twodimension topolog two neuron k lower case euclidean distanc determin two dimension space equival pythagoream theorem upper case simpli count discret path length k simplifi matter requir fix grid edg length case order abl output larg valu neighbor small valu neighbor function h need kind distanc notion grid somewher know far k apart grid differ method calcul distanc two dimension grid could appli instanc euclidean distanc lower part fig one dimension grid could simpli use number connect neuron k upper part figur definit topolog function topolog function h k describ neighborhood relationship topolog unimod function reach maximum k gilt time depend option often use introduct common distanc topolog function common distanc function would exampl alreadi known gaussian bell see fig next page unimod maximum close addit width chang appli paramet use realiz neighborhood reduc cours time simpli relat time depend result monoton decreas topolog function could look like h k gi ck gi gk repres neuron posit grid neuron posit input space would refer ck function use instead gaussian function instanc cone function cylind function mexican function fig follow page mexican function offer particular biolog motiv due negat digit reject neuron close winner neuron behavior alreadi observ natur caus sharpli separ map area exact mexican function suggest teuvo kohonen adjust characterist necessari function map could even possibl map would diverg could virtual explod learn rate neighborhood decreas monoton time avoid later train phase forc pull entir map toward new pattern som often work tempor monoton decreas learn rate neighborhood size first let us talk learn rate typic size target valu learn rate two size smaller initi valu g could true size must depend network topolog size neighborhood alreadi seen decreas neighborhood size realiz exampl mean time depend monoton decreas gaussin bell use topolog function h r r gaussian f x x cone function f x x cylind funktion f x x mexican function figur gaussian bell cone function cylind function mexican function suggest kohonen exampl topolog function advantag decreas neighborhood size begin move neuron pull along mani neuron vicin random initi network unfold fast proper begin learn process neuron influenc time stiffen network whole enabl good fine tune individu neuron must note h must alway true sinc otherwis neuron would constant miss current train sampl enough theori let us take look action exampl function som let us begin simpl mental comprehens exampl exampl use two dimension input space true let grid structur one dimension g furthermor exampl consist neuron learn rate neighborhood function kept simpl abl mental comprehend network h k k direct neighbor k otherw let us take look mention network random initi center fig next page enter train sampl p obvious exampl input pattern closest neuron win neuron rememb learn rule som ck h k p ck process three factor back learn direct rememb neuron center ck vector input space well pattern p thus factor p ck indic vector neuron k pattern p multipli differ scalar p figur illustr two dimension input space left one dimension topolgi space right self organ map neuron winner neuron sinc closest p topolog neuron neighbor arrow mark movement winner neuron neighbor toward train sampl p illustr one dimension topolog network plot input space dot line arrow mark movement winner neuron neighbor toward pattern topolog function h indic winner neuron two closest neighbor allow learn return neuron time depend specifi thus vector p ck multipli either learn rate indic alway strength learn alreadi mention result winner neuron neighbor approxim pattern p half way figur mark arrow although center neuron seen input space consider closer input pattern p neuron neuron learn neuron remind network topolog specifi neuron allow learn posit input space exact mechan topolog signific cover input space without relat sort adapt neuron next pattern appli anoth exampl one dimension develop two dimension input space uniform distribut input pattern cours time seen figur follow page state one two dimension som differ shape input space seen figur page see everi input space neat cover everi network topolog call expos neuron neuron locat area input pattern ever occur onedimension topolog general produc less expos neuron two dimension one instanc train circular arrang input pattern near imposs two dimension squar topolog avoid expos neuron center circl pull everi direct train final remain center make one dimension topolog optim topolog sinc find less complex neighborhood relationship multi dimension one topolog defect failur unfold unfold could happen topolog defect fig page occur unfold correct topolog defect describ best mean word knot remedi topolog defect could increas initi valu neighborhood size complex topolog neighbor figur behavior one dimension topolog g input random distribut input pattern p r train decreas paramet gauss function decreas figur state one dimension left column two dimension right column som differ input space neuron use one dimension topolog neuron two dimensions topolog input pattern map figur topolog defect two dimension neuron respect sinc three dimension honeycomb two dimension topolog could generat difficult random initi map unfold possibl adjust resolut certain area seen train enter input pattern input space r one anoth align pattern map could happen certain subset input space map precis one problem easili solv mean som train disproport mani input pattern area present number train pattern r present exceed number pat figur train g two dimension input space left side chanc becom train pattern equal coordin input space right side central circl input space chanc ten time larger remain input space visibl larger pattern densiti background circl neuron obvious crowd remain area cover less dens case neuron still even distribut two som train mean train sampl decreas well decreas tern remain r neuron group remain neuron spars distribut r fig see illustr edg could deform compens assign edg input space slight higher probabl hit train pattern often appli approach reach everi corner som higher learn rate often use edg corner neuron sinc pull center topolog result signific improv corner coverag applic som regard biolog inspir associ data storag mani field applic self organ map variat exampl differ phonem finnish languag success map onto two dimension discret grid topolog therefor neighborhood found noth els find neighborhood relationship one tri break high dimension space low dimension space topolog look structur develop voil clear defin area individu phenomenon form teuvo kohonen made effort search mani paper mention som keyword larg input space individu paper individu posit depend occurr keyword kohonen creat g use map high dimension paper space develop thus possibl enter paper complet train look neuron activ like discov neighbor paper topolog interest type brain like context base search work mani input space note system defin neighbor similar within topolog interest exampl show posit neuron input space signifi cant rather interest see neuron activ unknown input pattern enter next look previous input neuron activ immedi discov group similar input input within topolog diverg less thing common virtual topolog generat map input characterist reduc descript dimens relat input dimens therefor topolog often two dimension easili visual input space high dimension som use determin center rbf neuron som arrang exact toward posit outgo input result use exampl select center rbf network alreadi introduc paradigm rbf network chapter alreadi seen possibl control area input space cover higher resolut connect rbf network area function rbf network work neuron work exact use featur combin rbf network som one use topolog obtain final train rbf neuron use influenc neighbor rbf neuron differ way mani neural network simul offer addit call layer connect simul rbf network variat som differ variat som differ variat represent task neural gas without static topolog neural gas variat self organ map thoma martinetz mbs develop difficulti map complex input inform partial occur subspac input space even chang subspac fig follow page idea neural gas rough speak realiz without grid structur due fact deriv som learn step similar learn step includ addit intermedi step random initi ck r select present pattern input space p r neuron distanc measur identif winner neuron intermedi step generat list neuron sort ascend order distanc winner neuron thus first neuron list neuron closest winner neuron chang center mean known rule slight modifi topolog function hl k figur figur fill differ subspac actual input space differ posit therefor hard fill function hl k slight modifi compar origin function h k regard first element list neighborhood winner neuron direct result similar free float molecul gas neighborhood relationship neuron chang anytim number neighbor almost arbitrari distanc within neighborhood repres distanc within input space bulk neuron becom stiffen mean constant decreas neighborhood size fix dimens take dimens local need moment advantag disadvantag could fix grid forc input space becom regular cover therefor whole occur cover neuron isol spite practic hint alway user respons understand text catalog easi answer explor advantag disadvantag unlik neighborhood neural gas must initi refer neuron sinc otherwis outlier random initi may never reach remain group forget popular error implement neural gas neural gas possibl learn kind complex input fig preced page sinc bound fix dimension grid comput effort could necessari perman sort list could effect store list order data structur right start definit neural gas neural gas differ complet dynam neighborhood function everi learn cycl decid anew neuron neigborhood neuron winner neuron general criterion decis distanc neurosn winner neuron input space multi consist sever separ som order present anoth variant som formul extend problem input pattern know confin differ mayb disjoint area idea use one sever one multi self organ map short refer gke b gke gs unnecessari som topolog size combin som learn process analog som howev neuron belong winner train step adapt thus easi repres two disjoint cluster data mean two som even one cluster repres everi dimens input space r actual individu som exact reflect cluster definit multi multi noth simultan use som multi neural gas consist sever separ neural gase analog multi set neural gase multi neural gas gs sg construct behav analog neural gas neuron winner gas adapt reader certain wonder advantag use multi neural gas sinc individu neural gas alreadi capabl divid cluster work complex input pattern chang dimens basic correct multi neural gas two serious advantag simpl neural gas sever gase direct tell neuron belong gas particular import cluster task multi neural gase use recent simpl neural gase find cover cluster cannot recogn neuron belong cluster lot comput effort save larg origin gase divid sever smaller one sinc alreadi mention sort list could use lot comput effort sort sever smaller list lm less time consum even list total contain number neuron result obtain local instead global sort case local sort suffici choos two extrem case multi neural gase one extrem case ordinari neural gas use one singl neural gas interest enough extrem case larg one neuron gas behav analog k mean cluster inform cluster procedur see excursus definit multi neural gas multi neural gas noth simultan use neural gase grow neural gase add neuron grow neural gas variat aforement neural gas neuron ad accord certain rule thus attempt work isol neuron generat larger whole cover subject mention discuss build grow difficult new neuron integr neighborhood exercis exercis regular two dimension grid shall cover two dimension surfac well possibl grid structur would suit best purpos criteria use well best imprecis formul exercis intent chapter adapt reson theori art network origin form shall classifi binari input vector assign output simultan far unclassifi pattern shall recogn assign new class smaller chapter tri figur basic idea adapt reson theori abbrevi art without discuss theori profound sever section alreadi mention difficult use neural network learn new inform addit without destroy alreadi exist inform circumst call stabil plastic dilemma stephen grossberg gail carpent publish first version art network gro order allevi problem follow whole famili art improv discuss briefli idea unsupervis learn whose aim initi binari pattern recognit precis categor pattern class addit art network shall capabl find new class task structur art network art network compris exact two layer input layer recognit layer input layer complet link toward recognit layer complet link induc top weight matrix w contain weight valu connect neuron input layer neuron recognit layer fig follow page gfed abc gfed abc gfed abc w gfed abc w gfed abc ee gfed abc oo ee gfed abc oo ee gfed abc oo ee gfed abc g oo gfed abc g figur simplifi illustr art network structur top input layer bottom recognit layer illustr later inhibit recognit layer control neuron omit simpl binari pattern enter input layer transfer recognit layer recognit layer shall return encod follow winner take scheme instanc realiz encod principl later inhibit use implement activ neuron search practic reason queri would suit task best reson take place activ toss turn exist bottom weight matrix v propag activ within recognit layer back input layer obvious activ bounc forth back fact lead us reson everi activ within input layer caus activ within recognit layer turn recognit layer everi activ caus activ within input layer addit two mention layer art network exist neuron exercis control function signal enhanc discuss theori sinc basic principl art network becom explicit mention explain spite recurr art network achiev stabl state input learn process art network divid top bottom learn trick adapt reson theori configur art network two piec learn procedur theori one hand train top matrix w hand train bottom matrix v fig next page pattern input top learn pattern enter network caus alreadi mention activ output neuron strongest neuron win weight matrix w go toward output neuron chang output strongest neuron still enhanc class affili input vector class output neuron becom enhanc reson bottom learn train backward weight matrix v bit tricki weight respect winner neuron train toward input layer current input pattern use teach input thus network train enhanc input vector ad output neuron cours could happen neuron near equal activ sever neuron activ network indecis case mechan control neuron activ signal add new output neuron current pattern assign output neuron weight set new neuron train usual kapitel adapt reson theori dkriesel gfed abc gfed abc gfed abc gfed abc gfed abc oo ee gfed abc b oo ee gfed abc f f f f f f f f f f f f f f f f f f f f gfed abc gfed abc gfed abc gfed abc oo ee gfed abc b oo ee gfed abc gfed abc gfed abc gfed abc gfed abc oo ee gfed abc fbfffffffffffffffffff oo ee abbildung vereinfacht darstellung zweigeteilten train art netz jeweil trainierten gewicht durchgezogen dargestellt nehmen muster wurd netz eingegeben zahlen markieren ausgaben oben sehen gewinnerneuron mitt gewicht gewinnerneuron trainiert unten gewicht gewinnerneuron eingangsschicht trainiert abfrag mechanismus neuronalen netz gepresst erweiterungen schon eingang erw ahnt wurden art netz vielfach erweitert art cg erweiterung kontinuierlich eingaben bietet zus atzlich art genannten erweiterung verbesserungen lerngeschwindigkeit zus atzlich kontrollneuron schichten folg art cg verbessert lernf ahigkeit art zus atzlich biologisch vorg ang z b chemischen vorg ang innerhalb synapsen adaptiert zus atzlich beschriebenen erweiterungen existieren viel mehr h aufigen erweiterungen adapt reson theori sprechen b ose zungen bereit art netzen kriesel kleiner uberblick uber neuronal netz epsilon figur simplifi illustr two piec train art network train weight repres solid line let us assum pattern enter network number mark output top see winner neuron middl weight train toward winner neuron weight winner neuron train toward input layer thus advantag system divid input class find new class tell us activ output neuron typic repres class look like signific featur often howev system moder distinguish pattern question new neuron permit becom activ learn art network differ addit control neuron answer question accord differ mathemat rule respons intercept special case time one largest object art fact art network use special distinct case similar queri forc mechan neural network extens alreadi mention art network often extend art cg extend continu input addit offer extens call art enhanc learn speed result addit control neuron layer art cg improv learn abil art adapt addit biolog process chemic process within synaps apart describ one exist mani extens frequent extens adapt reson theori wag tongu alreadi call art network part iv excursi appendic regist appendix excursus cluster analysi region onlin learnabl field grimm dictionari extinct german word kluster describ dicht dick zusammensitzet thick dens group sth static cluster analysi format group within point cloud explor introduct procedur comparison advantag disadvantag discuss adapt cluster method base neural network region onlin learnabl field model point cloud possibl lot point compar small set neuron repres point cloud alreadi mention mani problem trace back problem cluster analysi therefor necessari research procedur examin whether group call cluster exist within point cloud sinc cluster analysi procedur need notion distanc two point metric must defin space point situat briefli specifi metric definit metric relat dist x x defin two object x x refer metric follow criteria appli dist x x x x dist x x dist x x symmetri dist x x dist x x dist x x triangl inequ hold colloqui speak metric tool determin distanc point space distanc symmetr distanc point may two point equal addit triangl inequ must appli metric provid exampl squar distanc euclidean distanc alreadi introduc base metric defin cluster procedur use metric distanc measur introduc briefli discuss differ cluster procedur k mean cluster alloc data predefin number cluster k mean cluster accord macqueen mac algorithm often use low comput storag complex regard inexpens good oper sequenc k mean cluster algorithm follow provid data examin defin k number cluster center select k random vector cluster center refer codebook vector assign data point next codebook vector comput cluster center cluster set codebook vector new cluster center continu assign longer chang step alreadi show one great question k mean algorithm number k cluster center determin advanc cannot done algorithm problem necessarili known advanc k determin best anoth problem procedur becom quit instabl codebook vector bad initi sinc random often use restart procedur advantag requir much comput effort fulli awar weak receiv quit good result name codebook vector creat often use name cluster vector unclear howev complex structur cluster cluster cannot recogn k high outer ring construct follow illustr recogn mani singl cluster k low ring small inner cluster recogn one cluster illustr see upper right part fig page k nearest neighbor look k nearest neighbor data point k nearest neighbor procedur ch connect data point k closest neighbor often result divis group group build cluster advantag number cluster occur disadvantag larg storag comput effort requir find next neighbor distanc data point must comput store special case procedur combin data point belong differ cluster k high see two small cluster upper right illustr cluster consist one singl data point basic connct anoth cluster alway intent furthermor mandatori link point symmetr procedur allow recognit ring therefor cluster cluster clear advantag anoth advantag procedur adapt respond distanc cluster illustr see lower left part fig nearest neighbor look neighbor within radius data point anoth approach neighbor neighborhood detect use fix number k neighbor radius reason name epsilonnearest neighbor point neigbor apart storag comput effort obvious high disadvantag note special case two separ cluster easili connect due unfavor situat singl data point happen k nearest neighbor would difficult sinc case number neighbor point limit advantag symmetr natur neighborhood relationship anoth advantag combin minim cluster due fix number neighbor avoid hand necessari skill initi order success smaller half smallest distanc two cluster variabl cluster point distanc within cluster possibl problem illustr see lower right part fig silhouett coeffici determin accur given cluster see easi answer cluster problem procedur describ specif disadvantag respect use criterion decid good cluster divis possibl offer silhouett coeffici accord kau coeffici measur well cluster delimit indic point may assign wrong cluster let p point cloud p point p let p cluster within point cloud p part cluster p set cluster call summari p p appli calcul silhouett coeffici initi need averag distanc point p cluster neighbor variabl refer p defin follow p x q q p dist p q figur top left set point use set explor differ cluster method top right k mean cluster use procedur chose k see procedur capabl recogn cluster cluster bottom left illustr long line point problem would recogn mani small cluster k suffici larg bottom left k nearest neighbor k select high higher number point smallest cluster result cluster combin shown upper right illustr bottom right nearest neighbor procedur caus difficulti select larger minimum distanc two cluster see upper left illustr combin furthermor let b p averag distanc point p point next cluster g repres cluster except b p g g g x q g dist p q point p classifi well distanc center cluster minim distanc center cluster maxim case follow term provid valu close p b p p max p b p appar whole term p within interv valu close indic bad classif p silhouett coeffici p result averag valu p p p x p p p total qualiti cluster divis express interv differ cluster strategi differ characterist present lot materi present dhs well measur indic qualiti exist arrang given data cluster introduc cluster method base unsupervis learn neural network sge publish like method one may perfect elimin larg standard weak known cluster method region onlin learnabl field neural cluster strategi paradigm neural network introduc region onlin learnabl field short refer rolf rolf tri cover data neuron rough speak region onlin learnabl field set k neuron tri cover set point well possibl mean distribut input space neuron ad move chang size train necessari paramet individu neuron discuss later definit region onlin learnabl field region onlin learnabl field abbrevi rolf rolf network set k neuron train cover certain set input space well possibl rolf neuron featur posit radius input space rolf neuron k k two paramet similar rbf network center ck posit input space yet anoth paramet radius defin radius percept surfac surround neuron neuron cover part input space situat within radius ck k local defin neuron particular mean neuron capabl cover surfac differ size radius percept surfac specifi r fig next page multipli global defin previous specifi neuron intuit reader wonder multipl use signific discuss later furthermor follow observ necessari percept surfac differ neuron size definit rolf neuron paramet rolf neuron k center ck radius k definit percept surfac percept surfac rolf neuron k consist point within radius input space rolf learn unsupervis present train sampl onlin like mani paradigm neural network rolf network learn receiv mani train sampl p train set p learn unsupervis train sampl p enter network two case occur one accept neuron k p accept neuron first case sever neuron suitabl exact one accept neuron insofar closest neuron accept one accept neuron k ck k adapt write defin actual radius specifi figur structur rolf neuron definit accept neuron criterion rolf neuron k accept neuron point p point p must locat within percept surfac k p locat percept surfac sever neuron closest neuron accept one sever closest neuron one chosen random posit radii adapt throughout learn let us assum enter train sampl p network accept neuron k radius move toward p ck toward distanc p ck center ck toward p addit let us defin two learn rate radii center ck ck p ck k k p ck k note k scalar ck vector input space definit adapt rolf neuron neuron k accept point p adapt accord follow rule ck ck p ck k k p ck k radius multipli allow neuron abl shrink understand function multipli due multipli percept surfac neuron includ point surround neuron radius mean due aforement learn rule cannot decreas increas definit radius multipli radius multipli global defin expand percept surfac neuron k multipl k ensur radius k cannot decreas increas general radius multipli set valu lower one digit rang far discuss case rolf train accept neuron train sampl p requir new neuron generat suggest discuss approach case accept neuron case new accept neuron k generat train sampl result cours ck k initi initi ck understood intuit center new neuron simpli set train sampl ck p generat new neuron neuron close p logic reason place neuron exact p set new neuron generat purpos exist differ option init alway select predefin static minimum take look neuron select minimum maximum take look neuron select maximum mean select mean neuron current mean variant favorit one although learn procedur work one minimum variant neuron tend cover less surfac maximum variant tend cover surfac definit generat rolf neuron new rolf neuron k generat enter train sampl p ck intial p k accord one aforement strategi init minimum maximum mean train complet repeat random permut pattern present new neuron generat epoch posit neuron bare chang evalu rolf result train algorithm train set gradual cover well precis rolf neuron high concentr point spot input space automat generat neuron thus possibl larg point cloud reduc repres base input set easi defin number cluster two neuron accord definit rolf connect percept surfac overlap kind nearest neighbor execut variabl percept surfac cluster group connect neuron group point input space cover neuron fig face page cours complet rolf network evalu mean cluster method neuron search cluster particular cluster method whose storag effort grow quadrat p storag effort reduc dramat sinc general consider less rolf neuron origin data point neuron repres data point quit well figur cluster process top input set middl input space cover rolf neuron bottom input space cover neuron repres comparison popular cluster method obvious store neuron rather store input point take biggest part storag effort rolf great advantag huge point cloud lot point sinc unnecessari store entir point cloud rolf neural cluster method capabl learn onlin definit great advantag furthermor similar nearest neighbor k nearest neighbor distinguish cluster enclos cluster due onlin present data without quadrat grow storag effort far greatest disadvantag two neighbor method addit issu size individu cluster proport distanc address use variabl percept surfac alway case two mention method rolf compar favor k mean cluster well first unnecessari previous know number cluster second k mean cluster recogn cluster enclos cluster separ cluster initi radii learn rate multipli trivial certain disadvantag rolf shall conceal alway easi select appropri initi valu previous knowledg data set say includ initi valu rolf fine grain data cluster use small small initi valu smaller smaller chanc neuron grow necessari easi answer like learn rate multipli lower singl digit rang popular success work valu variat run time imagin type network initi valu general depend cluster data distribut often test compar wrong initi least mean strategi relat robust train time whole rolf cluster method particular interest system low storag capac huge data set applic exampl first applic exampl could find color cluster rgb imag anoth field applic direct describ rolf public recognit word transfer dimension featur space thus see rolf relat robust higher dimens applic found field analysi attack network system classif exercis exercis determin least four adapt step one singl rolf neuron k four pattern state present one anoth indic order let initi valu rolf neuron ck k furthermor let let p appendix b excursus neural network use predict discuss applic neural network look ahead futur time seri discuss differ paradigm neural network use take look applic neural network brought often see use fraud applic time seri predict excursus structur descript time seri estim requir actual need predict valu time seri final say someth rang softwar predict share price econom characterist mean neural network procedur chapter detail descript rather indic approach time seri predict respect tri avoid formal definit b time seri time seri seri valu discret time exampl daili measur temperatur valu meteorolog data specif site could repres time seri share price valu repres time seri often measur time seri time equidist mani time seri futur develop valu interest g daili weather forecast time seri valu actual continu function read certain distanc time fig b next page figur b function x depend time sampl discret time step time discret mean result time seri sampl valu enter neural network exampl slp shall learn predict futur valu time seri predict time seri look neural network map previous seri valu futur develop time seri know longer section time seri enough train sampl cours exampl futur predict tri general extrapol past mean said sampl begin predict time seri answer question time seri deal ensur fulfil requir evid suggest futur valu depend way past valu time seri past time seri includ inform futur enough past valu time seri use train pattern case predict continu function must use look like question shall explor detail much inform futur includ past valu time seri import question answer time seri map futur futur valu time seri instanc depend past valu time seri predict base imposs chapter assum system whose futur valu deduc state determinist system lead us question system state system state complet describ system certain point time futur determinist system would clear defin mean complet descript current state problem real world state concept includ thing influenc system mean case weather forecast specif site could definit determin temperatur atmospher pressur cloud densiti meteorolog state place time whole state would includ signific inform worldwid phenomena control weather would interest well small local pheonomena cool system local power plant xt xt xt xt x predictor kk figur b represent one step ahead predict tri calcul futur valu seri past valu predict element case neural network refer predictor shall note system state desir predict alway possibl obtain often fragment current state acquir g weather forecast fragment said weather data howev partial overcom weak use one singl state last one predict use sever past state deriv first predict system b one step ahead predict first attempt predict next futur valu time seri past valu call one step ahead predict fig b predictor system receiv last observ state part system input output predict next state state part idea state space predict state call state space forecast aim predictor realiz function f xt xt xt xt b receiv exact past valu order predict futur valu predict valu shall head tild g x distinguish actual futur valu intuit simplest approach would find linear combin x xi xi ajxi b approxim fulfil condit construct call digit filter use fact time seri usual lot past valu set seri equat xt xt ajxt xt xt ajxt b xt xt ajxt thus equat could found unknown coeffici solv possibl anoth better approach could use equat unknown way sum mean squar error alreadi known predict minim call move averag procedur linear structur correspond singlelay perceptron linear activ function train mean data past experiment setup would compli fig b page fact train mean delta rule provid result close analyt solut even approach often provid satisfi result seen mani problem cannot solv use singlelay perceptron addit layer linear activ function useless well sinc multilay perceptron linear activ function reduc singlelay perceptron consider lead linear approach multilay perceptron linear activ function provid univers nonlinear function approxim use h mlp input past rbf network could use rememb number remain low sinc rbf network high input dimens complex realiz includ mani past valu multilay perceptron requir consider less comput effort b two step ahead predict approach use see farther futur without go detail remark predict becom easier past valu time seri avail would like ask reader read nyquist shannon sampl theorem predictor xt xt xt xt x oo x predictor jj figur b represent two step ahead predict attempt predict second futur valu past valu seri mean second predictor involv alreadi predict valu b recurs two step ahead predict order extend predict instanc two time step futur could perform two one step ahead predict row fig b recurs two step ahead predict unfortun valu determin mean onestep ahead predict general imprecis error built predict perform row imprecis becom result b direct two step ahead predict alreadi guess exist better approach like system train predict next valu certain train predict next one valu mean direct train exampl neural network look two time step ahead futur refer direct two stepahead predict fig b next page obvious direct two step ahead predict technic ident one step ahead predict differ train xt xt xt xt x x predictor ee figur b represent direct two step ahead predict second time step predict direct first one omit technic differ one step ahead predict b addit optim approach predict possibl predict valu far away futur import tri look farther ahead futur period time seri approach hard possibl lectur begin everi thursday use know mani peopl sat lectur room monday predict number lectur particip appli exampl period occur commut jam b chang tempor paramet thus use intent leav gap futur valu well past valu time seri introduc paramet indic past valu use predict technic speak still use one step ahead predict extend input space train system predict valu lie farther away possibl combin differ case traffic jam predict monday valu last day could use data input addit valu previous monday thus use last valu sever period case valu week daili period could includ annual period form begin holiday sure everyon us alreadi spent lot time highway forgot begin holiday xt xt xt xt x predictor kk yt yt yt yt figur b represent heterogen one step ahead predict predict time seri consider second one xt xt xt xt x predictor kk yt yt yt yt figur b heterogen one step ahead predict two time seri time b heterogen predict anoth predict approach would predict futur valu singl time seri sever time seri assum addit time seri relat futur first one heterogen one step ahead predict fig b predict two output two relat time seri certain possibl perform two parallel one step ahead predict analyt done often otherwis equat would becom confus case neural network addit output neuron attach knowledg time seri use output fig b find general materi time seri wg b remark predict share price mani peopl observ chang share price past tri conclud futur valu order benefit knowledg share price discontinu therefor princip difficult function furthermor function use discret valu often exampl daili rhythm includ maximum minimum valu day lucki daili variat certain elimin make whole thing even difficult chartist peopl look mani diagram decid mean lot background knowledg decad long experi whether equiti bought often success apart share price interest predict exchang rate currenc exchang euro dollar dollar pound pound back euro could possibl final receiv euro found would often thus would chang exchang rate state increas circul would longer possibl otherwis could produc money generat speak financi perpetu motion machin stock exchang success stock currenc broker rais lower thumb therebi indic whether opinion share price exchang rate increas decreas mathemat speak indic first bit sign first deriv exchang rate way excel worldclass broker obtain success rate great britain heterogen one step ahead predict success use increas accuraci predict addit time seri valu indic oil price rotterdam us nation debt includ exampl show magnitud accuraci stock exchang evalu sinc still talk first bit first deriv still know strong expect increas decreas whether effort pay probabl one wrong predict could nullifi profit one hundr correct predict neural network use predict share price intuit assum futur share price function previous share valu assumpt wrong share price function past valu function assum futur valu buy share valu increas last day believ futher increas tomorrow consequ mani peopl buy share boost price therefor assumpt right self fulfil propheci generat phenomenon long known econom appli way around sell share believ tomorrow price decreas beat price next day general even day next softwar appear use scientif key word neural network purport capabl predict share price go buy softwar addit aforement scientif exclus one simpl reason tool work manufactur sell normal use econom knowledg kept secret knew way definit gain wealth mean share would earn million use knowledg instead sell euro appendix excursus reinforc learn train sampl would nevertheless possibl evalu well learn solv problem let us examin learn paradigm situat supervis unsupervis learn introduc exot approach learn leav usual path know learn procedur network exact told provid exemplari output valu know learn procedur like self organ map input valu enter explor someth learn paradigm reinforc learn reinforc learn accord sutton barto sb reinforc learn neural network one three learn paradigm alreadi mention chapter sourc count among supervis learn procedur sinc feedback given due rudimentari feedback reason separ supervis learn procedur apart fact train sampl general known procedur backpropag cannot work human brain reinforc learn usual consid biolog motiv term reinforc learn come cognit scienc psycholog describ learn system carrot stick occur everywher natur learn mean good bad experi reward punish learn aid exact explain receiv total result process win game chess sure victori result individu intermedi step exampl ride bike worn tire speed exact km h turn sand grain size mm averag nobodi could tell us exact handlebar angl adjust even wors strong great number muscl part arm leg contract depend whether reach curv unharm soon face learn experi feedback reward good bad thus reward simpl hand consider easier obtain test differ veloc turn angl often enough receiv reward get feel work aim reinforc learn maintain exact feel anoth exampl quasi imposs achiev sort cost util function tenni player tri maxim athlet success long term mean complex movement ballist trajectori three dimension space includ wind direct import tournament privat factor mani get straight point sinc receiv littl feedback reinforc learn often mean trial error therefor slow system structur briefli discuss differ size compon system defin precis follow section broad speak reinforc learn repres mutual interact agent environment system fig agent shall solv problem could instanc autonom robot shall avoid obstacl agent perform action within environ return receiv feedback environ follow call reward cycl action reward characterist reinforc learn agent influenc system system provid reward chang reward real discret scalar describ mention well achiev aim give guidanc achiev aim alway make sum reward high possibl long term gridworld learn exampl reinforc learn would like use call gridworld see structur simpl easi figur therefor reinforc actual necessari howev suitabl repres approach reinforc learn let us exemplari defin individu compon reinforc system mean gridworld later compon examin exact environ gridworld fig follow page simpl discret world two dimens follow use environment system agent agent use simpl robot situat gridworld state space see gridworld field field unaccess therefor agent occupi posit grid world posit regard state agent action space action still miss simpli defin robot could move one field right left long obstacl edg gridworld task agent task leav gridworld exit locat right light color field determin two obstacl connect close lower part illustr correspond field inaccess posit cannot chang cycl cycl creat small world accompani us follow learn strategi illustr agent environ aim agent learn happen mean reward thus train mean dynam system environ order reach aim learn mean context agent shall learn map situat action call polici shall learn situat achiev certain given aim aim simpli shown agent give award achiev figur graphic represent gridworld dark color cell obstacl therefor inaccess exit locat right side light color field symbol mark start posit agent upper part figur open lower part close agent action environ reward new situat figur agent perform action within environ return receiv reward award must mistaken reward agent way solut may sometim use receiv smaller award punish return longterm result maximum similar situat investor sit downturn share price pawn sacrific chess game agent head right direct toward target receiv posit reward receiv reward even negat reward punish award speak final sum reward call return colloqui name basic compon discuss precis compon use make abstract reinforc learn system gridworld gridworld agent simpl robot find exit gridworld environ gridworld discret gridworld definit agent reinforc learn agent formal describ map situat space action space st mean situat st defin later indic action space depend current situat agent st definit environ environ repres stochast map action current situat st reward rt new situat st environ p rt state situat action alreadi mention agent differ state case gridworld exampl differ posit get two dimension state vector agent alway possibl realiz inform current state introduc term situat situat state agent point view less precis approxim state therefor situat general allow clear predict successor situat even complet determinist system may applic knew state transit exact thus complet system would possibl plan optim easi find optim polici method provid exampl dynam program know reinforc learn interact agent system includ action situat st agent cannot determin whether current situat good bad exact reason receiv said reward environ gridworld state posit agent situat simpli said situat equal state gridworld possibl action would move toward north south east west situat action vectori reward alway scalar extrem case even binari valu sinc aim reinforc learn get along littl feedback complex vectori reward would equal real teach input way cost function minim would possibl howev vectori reward sinc intuit order relat multi dimension space direct know better wors definit state within environ agent state state contain inform agent within environment system thus theoret possibl clear predict successor state perform action within determinist system godlik state knowledg definit situat situat st time situat space agent limit approxim knowledg state approxim agent cannot even know good make clear predict imposs definit action action perform agent whereupon could possibl depend situat anoth action space exist caus state transit therefor new situat agent point view reward return real life aim receiv award high possibl maxim sum expect reward r call return r long term finit mani time step reward simpli ad rt rt rt x x rt x certain return estim knew reward therefor return complet would longer necessari learn definit reward reward rt scalar real discret even sometim binari reward punish environment system return agent reaction action definit return return rt accumul receiv reward time deal long period time howev everi problem explicit target therefor finit sum g agent robot task drive around avoid obstacl order receiv diverg sum case infinit seri reward estim weaken factor use weaken influenc futur reward use exist target target far away rt rt rt rt x x x rt x farther reward away smaller influenc agent decis practic finit mani time step possibl even though formula state infinit sum first place anoth possibl handl return sum would limit time horizon mani follow reward rt rt regard rt rt rt x x x rt x thus divid timelin episod usual one two method use limit sum method togeth daili live tri approxim current situat desir state sinc mandatori next expect reward expect total sum decid agent possibl perform action short notic result negat reward g pawn sacrific chess game pay later polici consid formal system compon reinforc learn actual aim still discuss reinforc learn agent learn polici p thus continu adjust map situat probabl p action perform situat polici defin strategi select action would maxim reward long term gridworld gridworld polici strategi accord agent tri exit gridworld definit polici polici map situat probabl perform everi action action space formal p basic distinguish two polici paradigm open loop polici repres open control chain creat initi situat sequenc action thus begin agent develop plan consecut execut without consid intermedi situat therefor action depend situat gridworld gridworld open loop polici would provid precis direct toward exit way given start posit abbrevi direct eeeen open loop polici sequenc action without interim feedback sequenc action generat start situat system known well truli open loop polici use success lead use result exampl know chess game well truli would necessari tri everi possibl move would time consum thus problem find altern open loop polici incorpor current situat action plan close loop polici close loop function manner speak environ influenc action agent respond input environ respect alreadi illustr fig close loop polici speak reactiv plan map current situat action perform gridworld close loop polici would respons current posit choos direct accord action particular obstacl appear dynam polici better choic select action perform two basic strategi examin exploit vs explor real life reinforc learn often question aris whether exisit knowledg will exploit new way explor initi discuss two extrem greedi polici alway choos way highest reward determin advanc way highest known reward polici repres exploit approach promis use system alreadi known contrast exploit approach aim explor approach explor system detail possibl path lead target found may promis first glanc fact success let us assum look way restaur safe polici would alway take way alreadi know matter unoptim long may tri explor better way anoth approach would explor shorter way everi even risk take long time unsuccess therefor final take origin way arriv late restaur realiti often combin method appli begin learn process research higher probabl exist knowledg exploit static probabl distribut possibl often appli gridworld find way gridworld restaur exampl appli equal learn process let us take look daili life action lead us one situat differ subsitu subsitu sub subsitu sens get situat tree link node must consid often sever way reach situat tree could accur refer situat graph leav tree situat system explor approach would search tree thorough possibl becom acquaint leav exploit approach would uner go best known leav analog situat tree creat action tree reward action within node adapt daili life learn exact reward strategi interest import question reward kind reward award sinc design reward signific control system behavior seen general daili life various action perform situat differ strategi evalu select situat learn seri action would lead target first principl explain follow indic extrem case design exampl reward reward similar reward chess game refer pure delay reward receiv reward game method alway advantag final say whether succes interim step allow estim situat win rt well r lose r reward strategi reward return leav situat tree pure negat reward rt system find rapid way reach target way automat favor one respect reward agent receiv punish anyth even noth result inexpens method agent reach target fast anoth strategi avoid strategi harm situat avoid rt situat receiv reward receiv negat reward agent agent avoid get close negat situat warn reward strategi unexpect consequ robot told way touch obstacl punish simpli stand still stand still punish drive small circl reconsid understand behavior optim fulfil return robot unfortun intend furthermor show especi small task solv better mean negat reward posit differenti reward use larg complex task gridworld appli pure negat reward strategi robot shall find exit fast possibl figur represent optim return field gridworld mean pure negat reward award top open bottom close state valu function unlik agent godlik view gridworld swift determin robot start posit provid optim return figur optim return appli field gridworld state valu function gridworld exact repres function situat posit differ function unknown learn thus see would practic robot capabl evalu current futur situat let us take look anoth system compon reinforc learn state valu function v regard polici often call v whether situat bad often depend general behavior agent situat bad polici search risk check limit would instanc agent bicycl turn corner front wheel begin slide due daredevil polici agent would brake situat risk awar polici situat would look much better thus would evalu higher good state valu function v simpli return valu current situat agent polici abstract speak accord definit valu statevalu function correspond return rt expect valu situat st denot set expect return current situat st v rt st definit state valu function state valu function v task determin valu situat polici answer agent question whether situat good bad good bad purpos return expect return situat v rt st optim state valu function call v unfortuna unlik us robot godlik view environ tabl optim return like one shown orient aim reinforc learn robot generat state valu function bit bit basi return mani trial approxim optim state valu function v one context introduc two term close relat cycl state valu function polici polici evalu polici evalu approach tri polici time provid mani reward way gradual accumul state valu function mean reward polici improv polici improv mean improv polici turn new better one order improv polici aim return final larger valu found shorter way restaur walk success v v figur cycl reinforc learn ideal lead optim v principl reinforc learn realiz interact tri evalu good polici individu situat chang state valu function provid inform system improv polici two valu lift mathemat prove final result optim polici optim state valu function v fig cycl sound simpl time consum first let us regard simpl random polici robot could slowli fulfil improv state valu function without previous knowledg mont carlo method easiest approach accumul state valu function mere trial error thus select random behav polici consid accumul statevalu function random decis prove point find exit gridworld chanc inspir random base game chanc approach call mont carlo method addit assum pure negat reward obvious receiv optimum valu start field state valu function depend random way random polici take valu smaller occur start field intuit memor better valu one state one field caution advis way learn procedur would work determinist system open close cycl would produc oscil field oscil would influenc shortest way target mont carlo method prefer use learn rule v st new v st rt v st updat state valu function obvious influenc old state valu receiv return learn rate thus agent get kind memori new find alway chang situat valu littl bit exemplari learn step shown fig next page exampl comput state valu appli one singl state initi state obvious possibl often done train valu state visit case gridworld way target time result calcul relat exampl illustr fig page mont carlo method seem suboptim usual signific slower follow method reinforc learn method one mathemat prove work therefor use theoret consider definit mont carlo learn action random perform regardless state valu function long term express state valu function accumul mean follow learn rule v st new v st rt v st tempor differ learn learn result experi g walk ride bicycl without get injur even mental skill like mathemat problem solv benefit lot experi simpl trial error thus initi polici arbitrari valu tri learn improv polici due experi fig contrast mont carlo method direct manner learn experi react differ situat differ way tempor differ learn abbrevi td learn train v agent learn estim situat worth lot current situat identifi st follow situat learn rule among other deriv mean bellman equat deriv discuss chapter figur applic mont carlo learn rule learn rate top two exemplari way agent random select appli one open one close bottom result learn rule valu initi state consid way due fact cours time mani differ way walk given random polici express state valu function obtain figur extens learn exampl fig return intermedi state use accumul state valu function low valu field seen well state possibl must posit close state imposs evalu q polici improv figur tri differ action within environ result learn improv polici st thus learn formula state valu function v st v st new v st rt v st v st z chang previous valu see chang valu current situat st proport learn rate influenc receiv reward rt previous return weight factor follow situat v st previous valu situat v st definit tempor differ learn unlik mont carlo method td learn look ahead regard follow situat st thus learn rule given v st new v st rt v st v st z chang previous valu action valu function analog state valu function v action valu function q anoth system compon reinforc learn evalu certain action certain situat polici gridworld gridworld action valu function tell us good move certain field certain direct fig next page definit action valu function like state valu function actionvalu function q st evalu certain action basi certain situat polici optim action valu function call q st shown fig action perform target situat refer achiev exist target situat otherwis action simpli perform figur exemplari valu action valu function posit move right one remain fastest way toward target move still quit fast way move good way provid open case gfed abc direct action gfed abc r k gfed abc r k onmlhijk r k gfed abc r direct reward h figur action perform desir target situat achiev attent paid number reward number begin action situat begin simpli adopt convent q learn impli q learn fomula action valu function analog td learn applic call q learn q st new q st rt max q st z greedi strategi q st z chang previous valu break chang current action valu proport learn rate current situat influenc receiv reward rt maximum action follow action weight greedi strategi appli sinc assum best known action select td learn hand mind alway get best known next situat previous valu action situat st known q st rememb weight mean usual action valu function learn consider faster state valu function must disregard reinforc learn general quit slow system find good advantag q learn initi arbitrarili mean q learn result alway q definit q learn q learn train action valu function mean learn rule q st new q st rt max q st q st thus find q case exampl applic td gammon td gammon success backgammon game base td learn invent gerald tesauro situat current configur board anyon ever play backgammon know situat space huge approx situat result state valu function cannot comput explicit particular late eighti td gammon introduc select reward strategi pure delay reward system receiv reward game time reward return system allow practic initi backgammon program entiti result achiev highest rank comput backgammon leagu strike disprov theori comput programm capabl master task better programm car pit let us take look car park one dimension road bottom deep pit without abl get slope side straight away mean engin power order leav pit trivial execut action possibl drive forward backward intuit solut think immedi move backward gain momentum opposit slope oscil way sever time dash pit action reinforc learn system would full throttl forward full revers noth everyth cost would good choic award reward system learn fast leav pit realiz problem cannot solv mean mere forward direct engin power system slowli build movement polici longer store tabl sinc state space hard discret polici function generat pole balanc pole balanc develop barto sutton anderson let given situat includ vehicl capabl move either right full throttl left full throttl bang bang control two action perform stand still imposs top car hing upright pole could tip side pole built way alway tip one side never stand still let us assum pole round lower angl pole relat vertic line refer furthermor vehicl alway fix posit x one dimension world veloc x one dimension world limit maximum valu minimum valu x adopt aim system learn steer car way balanc pole prevent pole tip achiev best avoid strategi long pole balanc reward pole tip reward interest system soon capabl keep pole balanc tilt suffi cientli fast small movement system most center space sinc farthest wall understand negat touch wall pole tip swing invert pendulum difficult system follow initi situat pole initi hang swung vehicl final stabil literatur task call swing invert pendulum reinforc learn connect neural network final reader would like ask text neural network includ chapter reinforc learn answer simpl alreadi introduc supervis unsupervis learn procedur although alway omnisci teacher make unsupervis learn possibl mean receiv feedback often someth kind critic school mark problem like solv mean reinforc learn everi problem easili solv like gridworld backgammon exampl approx situat situat tree larg branch factor let alon game tabl use gridworld longer realiz state action valu function thus find approxim function learn approxim reinforc learn compon immedi mind exact neural network exercis exercis robot control system shall persuad mean reinforc learn find strategi order exit maze fast possibl could appropri state valu function look like would generat appropri reward assum robot capabl avoid obstacl time know posit x orient exercis describ function two compon ase ace propos barto sutton anderson control pole balanc bibliographi bsa exercis indic sever classic problem informat could solv effici mean reinforc learn pleas give reason answer bibliographi jame anderson simpl neural network generat interact memori mathemat bioscienc apz anguita g parodi r zunino speed improv backpropag current generat workstat wcnn portland world congress neural network juli oregon convent center portland oregon volum lawrenc erlbaum bsa barto r sutton anderson neuron like adapt element solv difficult learn control problem ieee transact system cybernet septemb cg g carpent grossberg art self organ stabl categori recognit code analog input pattern appli optic cg cohen grossberg absolut stabil global pattern format parallel memori storag competit neural network comput societi press technolog seri neural network page cg g carpent grossberg art hierarch search use chemic transmitt self organis pattern recognit architectur neural network ch cover p hart nearest neighbor pattern classif ieee transact inform theori cr campbel jb reec biologi spektrum akademisch verlag cyb g cybenko approxim superposit sigmoid function mathemat control signal system mcss dhs r duda p hart g stork pattern classif wiley new york elm jeffrey elman find structur time cognit scienc april fah fahlman empir sudi learn speed back propag network technic report cmu cs cmu fmi k fukushima miyak ito neocognitron neural network model mechan visual pattern recognit ieee transact system cybernet septemb octob fri b fritzk fast learn increment rbf network neural process letter gke goerk f kintzler r eckmil self organ classif chaotic domain nonlinearattractor neural network proceed ijcnn intern joint confer volum gke b goerk f kintzler r eckmil self organ partit chaotic attractor control lectur note comput scienc page gro grossberg adapt pattern classif univers recod parallel develop code neural featur detector biolog cybernet gs nil goerk alexandra scherbart classif use multi som multi neural gas ijcnn page donald hebb organ behavior neuropsycholog theori wiley new york hop john hopfield neural network physic system emerg collect comput abil proc nation academi scienc usa hop jj hopfield neuron grade respons collect comput properti like two state neuron proceed nation academi scienc ht jj hopfield dw tank neural comput decis optim problem biolog cybernet jor jordan attractor dynam parallel connectionist sequenti machin proceed eighth confer cognit scienc societi page erlbaum kau kaufman find group data introduct cluster analysi find group data introduct cluster analysi wiley new york koh kohonen correl matrix memori ieeetc koh teuvo kohonen self organ format topolog correct featur map biolog cybernet koh teuvo kohonen self organ associ memori springerverlag berlin third edit koh kohonen self organ map neurocomput ksj r kandel h schwartz jessel principl neural scienc appleton lang lcds cun denker solla optim brain damag touretzki editor advanc neural inform process system page morgan kaufmann mac macqueen method classif analysi multivari observ proceed fifth berkeley symposium mathemat statist probabl vol page mbs thoma martinetz stanislav g berkovich klaus schulten neural gas network vector quantize applic timeseri predict ieee tran neural network mbw k micheva b buss weiler rourk smith singlesynaps analysi divers synaps popul proteom imag method marker neuron mp w mcculloch w pitt logic calculus idea imman nervous activ bulletin mathemat biolog mp minski papert perceptron press cambridg mass mr mcclelland rumelhart parallel distribut process explor microstructur cognit volum press cambridg david r parker optim algorithm adapt network second order back propag second order direct propag second order hebbian learn maureen caudil charl butler editor ieee first intern confer neural network icnn volum ii page ii ii san diego ca june ieee pg poggio f girosi theori network approxim learn press cambridg mass pin f pineda general back propag recurr neural network physic review letter pm w pitt w mcculloch know univers percept auditori visual form bulletin mathemat biolog pre prechelt proben set neural network benchmark problem benchmark rule technic report rb riedmil h braun direct adapt method faster backpropag learn rprop algorithm neural network ieee intern confer page ieee rd g roth dick evolut brain intellig trend cognit scienc rhw rumelhart g hinton r william learn represent back propag error natur octob rhw b david rumelhart geoffrey hinton r william learn intern represent error propag rumelhart mcclelland pdp research group editor parallel distribut process explor microstructur cognit volum foundat press rie riedmil rprop descript implement detail technic report univers karlsruh ros f rosenblatt perceptron probabilist model inform storag organ brain psycholog review ros f rosenblatt principl neurodynam spartan new york sb r sutton g barto reinforc learn introduct press cambridg sg scherbart goerk unsupervis system discov pattern time seri sge rolf schatten nil goerk rolf eckmil region onlin learnabl field sameer singh maneesha singh chidanand apt petra perner editor icapr volum lectur note comput scienc page springer ste k steinbuch lernmatrix kybernetik biolog cybernet vdm malsburg self organ orient sensit cell striat cortex kybernetik p wasserman neural comput theori practic new york nostrand reinhold wer p werbo beyond regress new tool predict analysi behavior scienc phd thesi harvard univers wer p werbo backpropag past futur proceed icnn san diego page wg weigend gershenfeld time seri predict addisonwesley wh b widrow hoff adapt switch circuit proceed wescon page wid r widner singl stage logic aiee fall general meet wasserman p neural comput theori practic nostrand reinhold zel andrea zell simul neuronal netz addison wesley german list figur robot sensor motor learn sampl exampl robot black box eight input two output institut field neural network central nervous system brain biolog neuron action potenti compound eye data process neuron various popular activ function feedforward network feedforward network shortcut direct recurr network indirect recurr network later recurr network complet link network exampl network without bias neuron exampl differ type neuron train sampl network capac learn curv differ scale gradient descent visual possibl error gradient descent spiral problem checkerboard problem perceptron three differ view singlelay perceptron singlelay perceptron sever output neuron singlelay perceptron error surfac network connect sketch xor slp two dimension linear separ three dimension linear separ xor network multilay perceptron output set posit inner neuron deriv backpropag illustr backpropag deriv momentum term fermi function hyperbol tangent function encod rbf network distanc function rbf network individu gaussian bell one two dimension space accumul gaussian bell one dimension space accumul gaussian bell two dimension space even coverag input space radial basi function uneven coverag input space radial basi function random uneven coverag input space radial basi function roessler attractor jordan network elman network unfold time hopfield network binari threshold function converg hopfield network fermi function exampl quantize exampl topolog exampl distanc topolog topolog function first exampl train one dimension topolog som one two dimension topolog differ input space topolog defect resolut optim certain area shape classifi neural gas structur art network learn process art network compar cluster analysi method rolf neuron cluster mean rolf b neural network read time seri b one step ahead predict b two step ahead predict b direct two step ahead predict b heterogen one step ahead predict b heterogen one step ahead predict two output gridworld reinforc learn gridworld optim return reinforc learn cycl mont carlo method extend mont carlo method improv polici action valu function reinforc learn timelin index step rule action action potenti action space action valu function activ activ function select adalin see adapt linear neuron adapt linear element see adapt linear neuron adapt linear neuron adapt reson theori agent algorithm amacrin cell approxim art see adapt reson theori art art art artifici intellig associ data storag atp attractor autoassoci axon b backpropag second order backpropag error recurr bar basi bias neuron binari threshold function bipolar cell black box brain brainstem capabl learn center rolf neuron neuron rbf neuron distanc central nervous system cerebellum cerebr cortex cerebrum chang weight cluster analysi cluster cns see central nervous system codebook vector complet linkag compound eye concentr gradient cone function connect context base search continu cortex see cerebr cortex visual cortic field associ primari cylind function dartmouth summer research project deep network delta delta rule dendrit tree depolar diencephalon see interbrain differ vector see error vector digit filter digit discret discret see quantize distanc euclidean squar dynam system earli stop electron brain elman network environ episod epoch epsilon nearest neighbor error specif total error function specif error vector evolutionari algorithm exploit approach explor approach exteroceptor f fastprop fault toler feedforward fermi function flat spot elimin fudg see flat spot elimin function approxim function approxim univers g ganglion cell gauss markov model gaussian bell general glial cell gradient gradient descent problem grid gridworld h heavisid function see binari threshold function hebbian rule general form heteroassoci hinton diagram histori develop hopfield network continu horizont cell hyperbol tangent hyperpolar hypothalamus individu eye see ommatidium input dimens input pattern input vector interbrain internod interoceptor interpol precis ion iri jordan network k k mean cluster k nearest neighbor layer hidden input output learnabl learn batch see learn offlin offlin onlin reinforc supervis unsupervis learn rate variabl learn strategi learn vector quantize len linear separ linear associ lock syndrom logist function see fermi function temperatur paramet lvq see learn vector quantize lvq lvq lvq see self organ map multi mark perceptron mathemat symbol see time concept see action space ep see error vector g see topolog see self organ map input dimens p see train set q see action valu function optim q see action valu function rt see return see situat space see temperatur paramet v see state valu function optim v see state valu function w see weight matrix wi see chang weight see polici see threshold valu see momentum see weight decay see delta see learn rate see rprop see rprop max see rprop see rprop see rprop see nabla oper see radius multipli err see error total err w see error function errp see error specif errp w see error function specif errwd see weight decay see action see center rbf neuron see neuron self organ map center see output dimens see input dimens p see train pattern rh see center rbf neuron distanc rt see reward st see situat see teach input wi see weight x see input vector see output vector fact see activ function fout see output function membran potenti memor metric mexican function mlp see perceptron multilay momentum momentum term mont carlo method moor penros pseudo invers move averag procedur myelin sheath nabla oper neocognitron nervous system network input neural gas grow multi neural network recurr neuron accept binari context fermi ident inform process input rbf output rolf self organ map tanh winner neuron layer see layer neurotransmitt node ranvier oligodendrocyt olvq neuron see bias neuron one step ahead predict heterogen open loop learn optim brain damag order activ asynchron fix order random order random permut order topolog order synchron output dimens output function output vector p parallel pattern see train pattern pattern recognit perceptron multilay recurr singlelay perceptron converg theorem perceptron learn algorithm period peripher nervous system person anderson f anderson jame anguita barto f carpent gail elman fukushima girosi grossberg stephen hebb donald hinton hoff marcian hopfield john ito jordan kohonen teuvo f lashley karl macqueen martinetz thoma mcculloch warren minski marvin f miyak nilsson nil papert seymour parker david pitt walter poggio pythagora riedmil martin rosenblatt frank rumelhart steinbuch karl sutton f tesauro gerald malsburg christoph werbo paul widrow bernard wightman charl william zuse konrad pinhol eye pns see peripher nervous system pole balanc polici close loop evalu greedi improv open loop pon propag function prune pupil q q learn quantize quickpropag r rbf network grow recept field receptor cell photo primari secondari recurr direct indirect later refractori period region onlin learnabl field reinforc learn repolar represent resili backpropag reson retina return reward avoid strategi pure delay pure negat rms see root mean squar rolf see region onlin learnabl field root mean squar rprop see resili backpropag saltatori conductor schwann cell self fulfil propheci self organ featur map self organ map multi sensori adapt sensori transduct shortcut connect silhouett coeffici singl lens eye singl shot learn situat situat space situat tree slp see perceptron singlelay snark snipe sodium potassium pump see self organ map soma spin spinal cord stabil plastic dilemma state state space forecast state valu function stimulus stimulus conduct apparatus surfac percept swing invert pendulum symmetri break synaps chemic electr synaps synapt cleft target td gammon td learn see tempor differ learn teacher forc teach input telencephalon see cerebrum tempor differ learn thalamus threshold potenti threshold valu time concept time horizon time seri time seri predict topolog defect topolog topolog function train pattern set train set transfer functionse activ function truncus cerebri see brainstem two step ahead predict direct unfold time v voronoi diagram w weight weight matrix bottom top weight vector weight sum widrow hoff rule see delta rule winner take scheme